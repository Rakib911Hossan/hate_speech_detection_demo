{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Rakib911Hossan/hate_speech_detection_demo/blob/main/TaskC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LOqdzcoxSbDN",
    "outputId": "5bd6fa57-17dd-4187-c598-dec83d2572f1"
   },
   "outputs": [],
   "source": [
    "# [Hate Speech Identification Shared Task](https://multihate.github.io/): Subtask 1C at [BLP Workshop](https://blp-workshop.github.io/) @IJCNLP-AACL 2025\n",
    "\n",
    "# This shared task is designed to identify the type of hate, its severity, and the targeted group from social media content.\n",
    "# In this subtask, given a Bangla text collected from YouTube comments, we need to predict:\n",
    "# 1. hate_type: Abusive, Sexism, Religious Hate, Political Hate, Profane, or None\n",
    "# 2. hate_severity: Little to None, Mild, or Severe\n",
    "# 3. to_whom: Individuals, Organizations, Communities, or Society\n",
    "\n",
    "### Downloading dataset from github\n",
    "!wget https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1C/blp25_hatespeech_subtask_1C_train.tsv\n",
    "!wget https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1C/blp25_hatespeech_subtask_1C_dev.tsv\n",
    "!wget https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1C/blp25_hatespeech_subtask_1C_dev_test.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Xo1UcrOSjLa",
    "outputId": "45ce050c-919f-4b87-fdb1-bf5825b3c18a"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install evaluate\n",
    "# !pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B8RZU20nSvky"
   },
   "outputs": [],
   "source": [
    "### Importing required libraries\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QEukOy7VS03P"
   },
   "outputs": [],
   "source": [
    "### Define file paths\n",
    "train_file = 'blp25_hatespeech_subtask_1C_train.tsv'\n",
    "validation_file = 'blp25_hatespeech_subtask_1C_dev.tsv'\n",
    "test_file = 'blp25_hatespeech_subtask_1C_dev_test.tsv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZfJNlakE8esU"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_file, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-9rlsqoz8fmu",
    "outputId": "6ca4a0fb-98bc-48d6-ee83-cf6355a154fc"
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Shape of dataset\n",
    "print(\"Shape:\", train_df.shape)\n",
    "\n",
    "# # Column names\n",
    "print(\"Columns:\", train_df.columns)\n",
    "\n",
    "# Data types & non-null counts\n",
    "print(train_df.info())\n",
    "\n",
    "# First few rows\n",
    "print(train_df.head())\n",
    "\n",
    "# Last few rows\n",
    "print(train_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yAszHOTt8waS",
    "outputId": "168d7989-712e-4e29-91f2-bccbc1a39bc7"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Numeric summary\n",
    "print(train_df.describe())\n",
    "\n",
    "# Categorical summary\n",
    "print(train_df.describe(include='object'))\n",
    "\n",
    "# Unique values per column\n",
    "for col in train_df.columns:\n",
    "    print(f\"{col}: {train_df[col].nunique()} unique values\")\n",
    "\n",
    "\n",
    "\n",
    "print(train_df.isnull().sum())  # Count missing values\n",
    "print(train_df.isna().mean()*100)  # Percentage of missing values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKbh7HoyS39I"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kRmxrnB67M_p",
    "outputId": "1e6363b4-ea36-4dd3-bb12-d10bfca1f671"
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "# üéØ OPTIMIZED CONFIGURATION BASED ON YOUR RESULTS\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./xlm_roberta_recovery/\",\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    # üîß ADJUSTED LEARNING SCHEDULE - Your model peaked early then overfitted\n",
    "    learning_rate=10e-6,                    # Lower LR for more stable convergence\n",
    "    num_train_epochs=8,                    # More epochs with early stopping\n",
    "    warmup_ratio=0.15,                     # Longer warmup for stability\n",
    "    lr_scheduler_type=\"cosine\",            # Smoother LR decay\n",
    "    # üõ°Ô∏è STRONGER REGULARIZATION - Combat the overfitting you observed\n",
    "    weight_decay=0.01,                     # Increase from your 0.01\n",
    "    max_grad_norm=0.5,                     # Tighter gradient clipping\n",
    "    dataloader_drop_last=True,             # More consistent batch sizes\n",
    "\n",
    "    # ‚úÖ KEEP YOUR SUCCESSFUL BATCH CONFIGURATION\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "\n",
    "    # üéØ EARLY STOPPING - Prevent the Epoch 4 overfitting you experienced\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_accuracy\",\n",
    "    greater_is_better=True,\n",
    "\n",
    "    # üìä COMPREHENSIVE MONITORING\n",
    "    logging_steps=250,                     # More frequent logging\n",
    "    eval_steps=None,                       # Eval every epoch\n",
    "    save_total_limit=3,                    # Keep only best 3 checkpoints\n",
    "\n",
    "    # üîß SYSTEM OPTIMIZATIONS\n",
    "    report_to=None,\n",
    "    dataloader_num_workers=2,\n",
    "    fp16=True,                            # Mixed precision for efficiency\n",
    "    group_by_length=True,                 # Batch similar lengths together\n",
    ")\n",
    "\n",
    "# üõë MANDATORY EARLY STOPPING - Based on your overfitting pattern\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=2,            # Stop after 2 epochs without improvement\n",
    "    early_stopping_threshold=0.001        # Minimum improvement threshold\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# üéØ OPTIMIZED DATA PARAMETERS\n",
    "max_train_samples = None\n",
    "max_eval_samples = None\n",
    "max_predict_samples = None\n",
    "max_seq_length = 512\n",
    "batch_size = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GITRaYHa7a0s",
    "outputId": "43b1915d-a7e6-4e66-fab4-00865ccdd3a0"
   },
   "outputs": [],
   "source": [
    "transformers.utils.logging.set_verbosity_info()\n",
    "\n",
    "log_level = training_args.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "    + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3lGgH97v7kC9",
    "outputId": "c91d3590-b6a1-4369-a5b8-e13cb3af56f5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "class MultiTaskClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_labels_dict):\n",
    "        super(MultiTaskClassifier, self).__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.encoder.config.hidden_size  # XLM-R hidden dim = 768\n",
    "\n",
    "        # Separate heads for each task\n",
    "        self.hate_type_head = nn.Linear(hidden_size, num_labels_dict['hate_type'])\n",
    "        self.hate_severity_head = nn.Linear(hidden_size, num_labels_dict['hate_severity'])\n",
    "        self.to_whom_head = nn.Linear(hidden_size, num_labels_dict['to_whom'])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # CLS token representation\n",
    "\n",
    "        hate_type_logits = self.hate_type_head(pooled_output)\n",
    "        hate_severity_logits = self.hate_severity_head(pooled_output)\n",
    "        to_whom_logits = self.to_whom_head(pooled_output)\n",
    "\n",
    "        return {\n",
    "            \"hate_type\": hate_type_logits,\n",
    "            \"hate_severity\": hate_severity_logits,\n",
    "            \"to_whom\": to_whom_logits\n",
    "        }\n",
    "\n",
    "# Setup\n",
    "model_name = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "num_labels_dict = {\n",
    "    \"hate_type\": 5,        # 5 classes\n",
    "    \"hate_severity\": 3,    # 3 classes\n",
    "    \"to_whom\": 4           # 4 classes\n",
    "}\n",
    "\n",
    "model = MultiTaskClassifier(model_name, num_labels_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmTYM-Q57qxU"
   },
   "outputs": [],
   "source": [
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mdIjA77XETUA",
    "outputId": "dc4ae086-01ed-4ad8-d582-61e239523a83"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# First, let's load and examine the basic structure\n",
    "print(\"=== LOADING AND BASIC INSPECTION ===\")\n",
    "try:\n",
    "    train_df = pd.read_csv(train_file, sep='\\t')\n",
    "    print(f\"‚úÖ Successfully loaded training data: {train_df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading training data: {e}\")\n",
    "    train_df = None\n",
    "\n",
    "if train_df is not None:\n",
    "    print(\"\\n=== COLUMN INFORMATION ===\")\n",
    "    print(f\"Total columns: {len(train_df.columns)}\")\n",
    "    print(f\"Column names: {list(train_df.columns)}\")\n",
    "    print(f\"Column data types:\")\n",
    "    for col in train_df.columns:\n",
    "        print(f\"  {col}: {train_df[col].dtype}\")\n",
    "\n",
    "    print(\"\\n=== FIRST FEW ROWS ===\")\n",
    "    print(train_df.head())\n",
    "\n",
    "    print(\"\\n=== NUMERIC SUMMARY ===\")\n",
    "    numeric_cols = train_df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(train_df[numeric_cols].describe())\n",
    "    else:\n",
    "        print(\"No numeric columns found\")\n",
    "\n",
    "    print(\"\\n=== CATEGORICAL SUMMARY ===\")\n",
    "    categorical_cols = train_df.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        print(train_df[categorical_cols].describe())\n",
    "    else:\n",
    "        print(\"No categorical columns found\")\n",
    "\n",
    "    print(\"\\n=== UNIQUE VALUES PER COLUMN ===\")\n",
    "    for col in train_df.columns:\n",
    "        unique_count = train_df[col].nunique()\n",
    "        print(f\"{col}: {unique_count} unique values\")\n",
    "\n",
    "        # Show unique values for columns with reasonable number of unique values\n",
    "        if unique_count <= 20:\n",
    "            unique_vals = train_df[col].unique()\n",
    "            print(f\"  Values: {unique_vals}\")\n",
    "        elif unique_count <= 100:\n",
    "            # Show first 10 unique values for medium-sized categories\n",
    "            unique_vals = train_df[col].unique()[:10]\n",
    "            print(f\"  First 10 values: {unique_vals}...\")\n",
    "\n",
    "    print(\"\\n=== MISSING VALUES ANALYSIS ===\")\n",
    "    missing_counts = train_df.isnull().sum()\n",
    "    missing_percentages = train_df.isna().mean() * 100\n",
    "\n",
    "    print(\"Missing value counts:\")\n",
    "    for col in train_df.columns:\n",
    "        count = missing_counts[col]\n",
    "        percentage = missing_percentages[col]\n",
    "        print(f\"  {col}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "    print(\"\\n=== COLUMN NAME MATCHING ANALYSIS ===\")\n",
    "    # Check for potential matches with expected columns\n",
    "    expected_columns = ['hate_type', 'hate_severity', 'to_whom']\n",
    "    actual_columns = [col.lower().strip() for col in train_df.columns]\n",
    "\n",
    "    print(\"Looking for matches with expected columns:\")\n",
    "    for expected in expected_columns:\n",
    "        print(f\"\\nLooking for '{expected}':\")\n",
    "\n",
    "        # Exact match\n",
    "        if expected in actual_columns:\n",
    "            original_col = train_df.columns[actual_columns.index(expected)]\n",
    "            print(f\"  ‚úÖ Exact match found: '{original_col}'\")\n",
    "        else:\n",
    "            # Partial matches\n",
    "            partial_matches = []\n",
    "            for i, actual in enumerate(actual_columns):\n",
    "                if expected.replace('_', '') in actual.replace('_', '') or actual in expected:\n",
    "                    partial_matches.append(train_df.columns[i])\n",
    "\n",
    "            if partial_matches:\n",
    "                print(f\"  üîç Possible matches: {partial_matches}\")\n",
    "            else:\n",
    "                print(f\"  ‚ùå No matches found\")\n",
    "\n",
    "    print(\"\\n=== SUGGESTED COLUMN MAPPING ===\")\n",
    "    # Create mapping suggestions\n",
    "    suggestions = {}\n",
    "\n",
    "    # Common variations for hate speech datasets\n",
    "    hate_type_patterns = ['type', 'class', 'category', 'hate', 'label']\n",
    "    severity_patterns = ['severity', 'level', 'intensity', 'degree']\n",
    "    target_patterns = ['target', 'whom', 'directed', 'group', 'victim']\n",
    "\n",
    "    for col in train_df.columns:\n",
    "        col_lower = col.lower().strip()\n",
    "\n",
    "        # Check hate_type patterns\n",
    "        for pattern in hate_type_patterns:\n",
    "            if pattern in col_lower and 'hate_type' not in suggestions:\n",
    "                suggestions['hate_type'] = col\n",
    "                break\n",
    "\n",
    "        # Check severity patterns\n",
    "        for pattern in severity_patterns:\n",
    "            if pattern in col_lower and 'hate_severity' not in suggestions:\n",
    "                suggestions['hate_severity'] = col\n",
    "                break\n",
    "\n",
    "        # Check target patterns\n",
    "        for pattern in target_patterns:\n",
    "            if pattern in col_lower and 'to_whom' not in suggestions:\n",
    "                suggestions['to_whom'] = col\n",
    "                break\n",
    "\n",
    "    print(\"Suggested mappings:\")\n",
    "    for expected, suggested in suggestions.items():\n",
    "        print(f\"  {expected} -> '{suggested}'\")\n",
    "\n",
    "    # Show unmapped columns\n",
    "    mapped_cols = set(suggestions.values())\n",
    "    unmapped_cols = [col for col in train_df.columns if col not in mapped_cols]\n",
    "    if unmapped_cols:\n",
    "        print(f\"\\nUnmapped columns: {unmapped_cols}\")\n",
    "\n",
    "    print(\"\\n=== SAMPLE VALUES FOR KEY COLUMNS ===\")\n",
    "    # Show sample values for likely label columns\n",
    "    for col in train_df.columns:\n",
    "        if train_df[col].nunique() <= 50:  # Likely categorical\n",
    "            print(f\"\\nSample values for '{col}':\")\n",
    "            value_counts = train_df[col].value_counts().head(10)\n",
    "            print(value_counts)\n",
    "\n",
    "    print(\"\\n=== RECOMMENDED NEXT STEPS ===\")\n",
    "    print(\"Based on this analysis:\")\n",
    "    print(\"1. Check the 'Suggested mappings' section above\")\n",
    "    print(\"2. Update your column names in the mapping code\")\n",
    "    print(\"3. Example fix:\")\n",
    "\n",
    "    if suggestions:\n",
    "        print(\"   # Instead of:\")\n",
    "        print(\"   # train_df['hate_type'] = train_df['hate_type'].map(hate_type2id)...\")\n",
    "        print(\"   # Use:\")\n",
    "        for expected, actual in suggestions.items():\n",
    "            print(f\"   train_df['{actual}'] = train_df['{actual}'].map({expected}2id).fillna(-1).astype(int)\")\n",
    "    else:\n",
    "        print(\"   # No automatic suggestions found. Please manually check column names.\")\n",
    "\n",
    "else:\n",
    "    print(\"Cannot proceed with analysis due to data loading error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KeyQ2-cB-2Bv",
    "outputId": "067eb353-c658-4f48-937f-44e0042513b4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Define label mapping dictionaries based on your data\n",
    "hate_type2id = {\n",
    "    'Abusive': 0,\n",
    "    'Political Hate': 1,\n",
    "    'Profane': 2,\n",
    "    'Religious Hate': 3,\n",
    "    'Sexism': 4\n",
    "}\n",
    "\n",
    "hate_severity2id = {\n",
    "    'Little to None': 0,\n",
    "    'Mild': 1,\n",
    "    'Severe': 2\n",
    "}\n",
    "\n",
    "to_whom2id = {\n",
    "    'Individual': 0,\n",
    "    'Organization': 1,\n",
    "    'Community': 2,\n",
    "    'Society': 3\n",
    "}\n",
    "\n",
    "# Create reverse mappings for reference\n",
    "id2hate_type = {v: k for k, v in hate_type2id.items()}\n",
    "id2hate_severity = {v: k for k, v in hate_severity2id.items()}\n",
    "id2to_whom = {v: k for k, v in to_whom2id.items()}\n",
    "\n",
    "print(\"Label mappings created:\")\n",
    "print(f\"Hate types: {hate_type2id}\")\n",
    "print(f\"Hate severity: {hate_severity2id}\")\n",
    "print(f\"Target groups: {to_whom2id}\")\n",
    "\n",
    "# Load datasets\n",
    "print(\"\\nLoading datasets...\")\n",
    "train_df = pd.read_csv(train_file, sep='\\t')\n",
    "valid_df = pd.read_csv(validation_file, sep='\\t')\n",
    "test_df = pd.read_csv(test_file, sep='\\t')\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Validation shape: {valid_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "\n",
    "# Function to safely apply label mapping\n",
    "def apply_label_mapping(df, column_name, mapping_dict, dataset_name):\n",
    "    \"\"\"Apply label mapping with detailed logging\"\"\"\n",
    "    if column_name not in df.columns:\n",
    "        print(f\"\\n‚ö†Ô∏è  Column '{column_name}' not found in {dataset_name} dataset\")\n",
    "        print(f\"Available columns: {list(df.columns)}\")\n",
    "        # Add dummy column with -1 values for missing labels (common in test sets)\n",
    "        df[column_name] = -1\n",
    "        print(f\"Added dummy column '{column_name}' with -1 values\")\n",
    "        return df\n",
    "\n",
    "    print(f\"\\nProcessing {column_name} in {dataset_name}...\")\n",
    "\n",
    "    # Check original distribution\n",
    "    original_counts = df[column_name].value_counts(dropna=False)\n",
    "    print(f\"Original {column_name} distribution:\")\n",
    "    print(original_counts)\n",
    "\n",
    "    # Apply mapping\n",
    "    df[column_name] = df[column_name].map(mapping_dict).fillna(-1).astype(int)\n",
    "\n",
    "    # Check final distribution\n",
    "    final_counts = df[column_name].value_counts(dropna=False)\n",
    "    print(f\"Mapped {column_name} distribution:\")\n",
    "    print(final_counts)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply mappings to all datasets\n",
    "datasets = [\n",
    "    (train_df, \"train\"),\n",
    "    (valid_df, \"validation\"),\n",
    "    (test_df, \"test\")\n",
    "]\n",
    "\n",
    "# Check test dataset structure first\n",
    "print(f\"\\nTest dataset columns: {list(test_df.columns)}\")\n",
    "print(\"Note: Test datasets often don't include labels for prediction tasks\")\n",
    "\n",
    "for df, name in datasets:\n",
    "    print(f\"\\n{'='*20} Processing {name} dataset {'='*20}\")\n",
    "\n",
    "    # Apply each mapping (will handle missing columns gracefully)\n",
    "    apply_label_mapping(df, 'hate_type', hate_type2id, name)\n",
    "    apply_label_mapping(df, 'hate_severity', hate_severity2id, name)\n",
    "    apply_label_mapping(df, 'to_whom', to_whom2id, name)\n",
    "\n",
    "# Convert to HuggingFace datasets\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Converting to HuggingFace datasets...\")\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "valid_ds = Dataset.from_pandas(valid_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Create DatasetDict\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": train_ds,\n",
    "    \"validation\": valid_ds,\n",
    "    \"test\": test_ds\n",
    "})\n",
    "\n",
    "print(\"‚úÖ Dataset creation successful!\")\n",
    "print(f\"Train samples: {len(train_ds)}\")\n",
    "print(f\"Validation samples: {len(valid_ds)}\")\n",
    "print(f\"Test samples: {len(test_ds)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3NW2zZOICTU",
    "outputId": "2495c25d-434a-4161-e6cd-da9dc0efeab3"
   },
   "outputs": [],
   "source": [
    "# Check test dataset size\n",
    "print(\"=== TEST DATASET INFO ===\")\n",
    "print(f\"Number of test samples: {len(test_df['id'])}\")\n",
    "print(f\"Test dataset shape: {test_df.shape}\")\n",
    "print(f\"Test dataset columns: {list(test_df.columns)}\")\n",
    "\n",
    "print(\"\\n=== EXTRACTING UNIQUE LABELS ===\")\n",
    "\n",
    "# Since you have multiple label columns (multi-task), we need to handle each separately\n",
    "label_columns = ['hate_type', 'hate_severity', 'to_whom']\n",
    "\n",
    "# Dictionary to store label information for each task\n",
    "label_info = {}\n",
    "\n",
    "for label_col in label_columns:\n",
    "    print(f\"\\n--- {label_col.upper()} LABELS ---\")\n",
    "\n",
    "    # Extract unique labels from training set (excluding -1 which represents missing values)\n",
    "    unique_labels = raw_datasets[\"train\"].unique(label_col)\n",
    "    print(f\"All unique values (including -1 for missing): {unique_labels}\")\n",
    "\n",
    "    # Filter out -1 (missing values) to get actual labels\n",
    "    actual_labels = [label for label in unique_labels if label != -1]\n",
    "    actual_labels.sort()  # Sort for consistency\n",
    "\n",
    "    print(f\"Actual labels (excluding missing): {actual_labels}\")\n",
    "    print(f\"Number of unique labels: {len(actual_labels)}\")\n",
    "\n",
    "    # Store information\n",
    "    label_info[label_col] = {\n",
    "        'labels': actual_labels,\n",
    "        'num_labels': len(actual_labels),\n",
    "        'all_values': unique_labels\n",
    "    }\n",
    "\n",
    "    # Show label distribution in training set\n",
    "    train_df_temp = raw_datasets[\"train\"].to_pandas()\n",
    "    label_dist = train_df_temp[label_col].value_counts().sort_index()\n",
    "    print(f\"Label distribution in training set:\")\n",
    "    for value, count in label_dist.items():\n",
    "        if value == -1:\n",
    "            print(f\"  Missing (-1): {count}\")\n",
    "        else:\n",
    "            # Get original label name\n",
    "            if label_col == 'hate_type':\n",
    "                original_name = id2hate_type.get(value, f\"Unknown({value})\")\n",
    "            elif label_col == 'hate_severity':\n",
    "                original_name = id2hate_severity.get(value, f\"Unknown({value})\")\n",
    "            else:  # to_whom\n",
    "                original_name = id2to_whom.get(value, f\"Unknown({value})\")\n",
    "            print(f\"  {original_name} ({value}): {count}\")\n",
    "\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "print(\"Label information for each task:\")\n",
    "for task, info in label_info.items():\n",
    "    print(f\"{task}:\")\n",
    "    print(f\"  Number of classes: {info['num_labels']}\")\n",
    "    print(f\"  Label range: {min(info['labels'])} to {max(info['labels'])}\")\n",
    "\n",
    "# Total number of labels across all tasks (if you need it)\n",
    "total_labels = sum(info['num_labels'] for info in label_info.values())\n",
    "print(f\"\\nTotal labels across all tasks: {total_labels}\")\n",
    "\n",
    "# Create variables for easy access (commonly used in model setup)\n",
    "num_hate_types = label_info['hate_type']['num_labels']\n",
    "num_hate_severities = label_info['hate_severity']['num_labels']\n",
    "num_to_whom = label_info['to_whom']['num_labels']\n",
    "\n",
    "print(f\"\\nVariables for model configuration:\")\n",
    "print(f\"num_hate_types = {num_hate_types}\")\n",
    "print(f\"num_hate_severities = {num_hate_severities}\")\n",
    "print(f\"num_to_whom = {num_to_whom}\")\n",
    "\n",
    "# If you need a single 'num_labels' variable (common for single-task models)\n",
    "# You would typically use this for the main classification task\n",
    "num_labels = num_hate_types  # Assuming hate_type is your primary task\n",
    "print(f\"num_labels (primary task - hate_type) = {num_labels}\")\n",
    "\n",
    "print(\"\\n=== VERIFICATION ===\")\n",
    "# Verify our mappings are consistent\n",
    "print(\"Checking consistency between mappings and extracted labels:\")\n",
    "for task, info in label_info.items():\n",
    "    if task == 'hate_type':\n",
    "        expected_labels = list(range(len(hate_type2id)))\n",
    "    elif task == 'hate_severity':\n",
    "        expected_labels = list(range(len(hate_severity2id)))\n",
    "    else:  # to_whom\n",
    "        expected_labels = list(range(len(to_whom2id)))\n",
    "\n",
    "    if set(info['labels']) == set(expected_labels):\n",
    "        print(f\"‚úÖ {task}: Mapping consistent\")\n",
    "    else:\n",
    "        print(f\"‚ùå {task}: Mapping inconsistent!\")\n",
    "        print(f\"   Expected: {expected_labels}\")\n",
    "        print(f\"   Found: {info['labels']}\")\n",
    "\n",
    "print(\"\\n=== DATASET SPLITS INFO ===\")\n",
    "for split_name, dataset in raw_datasets.items():\n",
    "    print(f\"{split_name.capitalize()} set: {len(dataset)} samples\")\n",
    "\n",
    "    # Show missing value percentages for each label column\n",
    "    df_temp = dataset.to_pandas()\n",
    "    for col in label_columns:\n",
    "        if col in df_temp.columns:\n",
    "            missing_count = (df_temp[col] == -1).sum()\n",
    "            missing_pct = (missing_count / len(df_temp)) * 100\n",
    "            print(f\"  {col} missing: {missing_count} ({missing_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d2bfa0c8f4d643348d2a3f2fb7048c3a",
      "260313e2c786483ebb0790969ddddfd3",
      "d3e129d6e85e42ecad4bba7413a8293f",
      "1b2600ea7a5b407ab9a6df2b3be5dc10",
      "e87885748c73443cb44187366d7443fc",
      "350bdb34f83d490ba6fe4a8c5a38eb87",
      "f3dfa03af9dc453da4b4ddf41f639678",
      "aed408ba569c4680a8cb219d0dde35cc",
      "38ecaa0b25f74259b018b4aae7c82094",
      "b62c986eec134621ac180274815a1908",
      "f12a2566a4af4ae0b89ac82155480363",
      "e3efcf306147497a9d285de711fc8c85",
      "10ab2cbbb05b4047b9445024d990e29f",
      "aab6315869ed4dfeaedce4198bdf32c2",
      "cac1adab334142d48821be4394fabd7f",
      "227bcc3c02f34b2aaa92ad75ef3ed680",
      "110fb49bd6154855bf7ea43be1fb661e",
      "d5fd762490d849c9b012b2f32bcc5e58",
      "2124f4b7e531412bb8bc9406e1b81d7d",
      "35a2f513e3fb4d9694d9be7660e2ded6",
      "8b6ffc55596446589f52ff1566846743",
      "4902bdd8bdd8440f811ebea0fbf0dc54",
      "3cbeddcedfd840b092ce42a88ce55dd7",
      "ab4a0c3ff9f94c2faa77b54271a5e80e",
      "794f1a862a374dd88b9eb99f3956646a",
      "77029186c15d47ab8711ef867c89fa41",
      "b1af33245b7f4849b0f9c9d0d13999ae",
      "f894279cb9034e51a1d3abcec466204f",
      "6505b949d5c847b3b4cfa82fb15d792d",
      "a2fa3d93c2de46d1b9e00a0774df4192",
      "7baf280ad0724e99a3a43cbada228216",
      "a5adcba0c76344f29a77e63d18ba7917",
      "65d5d3ee5bcb44b29ac630ea7e5ae7a4",
      "14dd1c58d6b340b8814715d45261ee2d",
      "328cda552afb4957b29cbbad28a7bcc8",
      "5b9fd27216e2429fbeb24e278d5abb39",
      "cfc90dafc3ba46e29733b3c6136e43ba",
      "a7b5e9ef038b4d398b37db4566034908",
      "7b8eacab11f04681a784d3616e89058d",
      "db1f3a8965e24dbbb56696f1b05a7faa",
      "7f6fc6c94aae4de18c4198cccd472251",
      "49522cce861c41d8af9f48851da6e4a9",
      "736fca0514a44584bef9ad3e19b08005",
      "043b3551df1f47eaa6ceb529d90ed8d0",
      "ab940c1823e14535b7bca1f359b24bb4",
      "1c5987dd212a4429a4eb6037b659c5d6",
      "d709384b114145edb8abf71d4e96561b",
      "79545c3d74bb458085394eb0ecc02f3b",
      "5b91a5c90f734f44908e21da883d3506",
      "a9060eaf7d1048e9b5c0ffed07a208a7",
      "b0d3c514cdbe4d149d7c842ab6167144",
      "54187cc7dcc9445e93a8774c47bd2655",
      "02a156b0f7954722bd2fff44715662a9",
      "4a651d6fe6384a708d25a844f83d4a65",
      "ecac1a0142624a9cbc7a6d63755efff0"
     ]
    },
    "id": "xgq6NePuJJjd",
    "outputId": "a43443d3-de7d-4f64-e5f5-335ca8372416"
   },
   "outputs": [],
   "source": [
    "# Define model name\n",
    "model_name = \"bert-base-multilingual-cased\"  # or your preferred model\n",
    "\n",
    "# For multi-task, we need custom config with multiple label counts\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_hate_types,  # Primary task (hate_type)\n",
    "    finetuning_task=None,\n",
    "    cache_dir=None,\n",
    "    revision=\"main\",\n",
    "    use_auth_token=None,\n",
    ")\n",
    "\n",
    "# Add custom attributes for multi-task\n",
    "config.num_hate_types = num_hate_types\n",
    "config.num_hate_severities = num_hate_severities\n",
    "config.num_to_whom = num_to_whom\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=None,\n",
    "    use_fast=True,\n",
    "    revision=\"main\",\n",
    "    use_auth_token=None,\n",
    ")\n",
    "\n",
    "# For multi-task, you'll need a custom model instead of AutoModelForSequenceClassification\n",
    "# But if you want to keep it simple, use the primary task (hate_type) for now:\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    from_tf=bool(\".ckpt\" in model_name),\n",
    "    config=config,\n",
    "    cache_dir=None,\n",
    "    revision=\"main\",\n",
    "    use_auth_token=None,\n",
    "    ignore_mismatched_sizes=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131,
     "referenced_widgets": [
      "32060cd4d71f4ac986d21048489a446c",
      "6267aa34a3f545b79944a384034605ed",
      "3e1a41590a1d445ca0a986ba87823dd0",
      "80d2a06deefa4d99833233a52340ba41",
      "9209d2c838b04363b89d2ed8014991e5",
      "0b720818d9184144a7b706c744ba909c",
      "849107ae303e42e0a9b6808ee5511b8a",
      "6e81b81552af47a7911b0c287f454407",
      "840278932b744e10bcec8297df237af7",
      "8e1c85b21ed4491b8acd46f6275eb2ec",
      "1b34186a06e34593995484006d462e51",
      "30af1ee39bd24f44b407c5d39374d5bb",
      "b967984e34d34d10a82b3d28ca0d5e74",
      "036641393b5a4b82b911492e39e2ac9a",
      "18388a1641004c2883cc94a835660fb6",
      "7e36bada1db4415ebcda02b405e4fc27",
      "45838e17e2e0432e928b2e3a70bf5034",
      "8774918a23c04125b079dd1c6bed5893",
      "7c4cde2784c245b590f041e80b5c3b28",
      "3d3cc56aa7e44002ab4f2fbf0a3dd0d9",
      "871ecb063f71438cae8cc1364d2dcb21",
      "52954b5c674b4742b2b9e35b78cd37db",
      "a7b2ad1165784721ab54bedf9d09b175",
      "1435a714d22b44f6bf7789add1e8570a",
      "2ea4d99a4f6048be9f837b08281f8522",
      "5272556add5b40c2aae70657cd257a70",
      "e2920528cc4a45a4990e13ebe0571d55",
      "d84278559bf24d158b27020d46fbb4fc",
      "e202c6cb29bb4ac385cc3877e4ce9a42",
      "4f72fff9754b4787a72f41dd76d68b55",
      "d3ecd21b7f934db78ed669fb2ce9b5d8",
      "27e2bbb76a1a41eb98fb8748d3389b59",
      "213cec2a75c04508b97a80cea5b8c7e7"
     ]
    },
    "id": "-6bktu_zJqXv",
    "outputId": "b8ce891c-6d97-418a-fa86-804b3bba26d4"
   },
   "outputs": [],
   "source": [
    "# Preprocessing the raw_datasets\n",
    "non_label_column_names = [name for name in raw_datasets[\"train\"].column_names if name not in [\"hate_type\", \"hate_severity\", \"to_whom\"]]\n",
    "sentence1_key = \"text\"  # Your text column\n",
    "\n",
    "# Padding strategy\n",
    "padding = \"max_length\"\n",
    "\n",
    "# For multi-task, skip the label mapping part since we handle multiple labels\n",
    "max_seq_length = min(128, tokenizer.model_max_length)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the texts\n",
    "    result = tokenizer(examples[sentence1_key], padding=padding, max_length=max_seq_length, truncation=True)\n",
    "\n",
    "    # Keep all label columns as they are\n",
    "    result[\"hate_type\"] = examples[\"hate_type\"]\n",
    "    result[\"hate_severity\"] = examples[\"hate_severity\"]\n",
    "    result[\"to_whom\"] = examples[\"to_whom\"]\n",
    "\n",
    "    return result\n",
    "\n",
    "raw_datasets = raw_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "# Training data\n",
    "train_dataset = raw_datasets[\"train\"]\n",
    "if 'max_train_samples' in locals() and max_train_samples is not None:\n",
    "    train_dataset = train_dataset.select(range(min(len(train_dataset), max_train_samples)))\n",
    "\n",
    "# Validation data\n",
    "eval_dataset = raw_datasets[\"validation\"]\n",
    "if 'max_eval_samples' in locals() and max_eval_samples is not None:\n",
    "    eval_dataset = eval_dataset.select(range(min(len(eval_dataset), max_eval_samples)))\n",
    "\n",
    "# Test data\n",
    "predict_dataset = raw_datasets[\"test\"]\n",
    "if 'max_predict_samples' in locals() and max_predict_samples is not None:\n",
    "    predict_dataset = predict_dataset.select(range(min(len(predict_dataset), max_predict_samples)))\n",
    "\n",
    "# Multi-task compute metrics\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    # For single task (hate_type only)\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\n",
    "\n",
    "# Data Collator\n",
    "data_collator = default_data_collator\n",
    "\n",
    "# Remove ID column for training\n",
    "train_dataset = train_dataset.remove_columns(\"id\")\n",
    "eval_dataset = eval_dataset.remove_columns(\"id\")\n",
    "\n",
    "# Since using AutoModelForSequenceClassification, we need to rename primary label\n",
    "train_dataset = train_dataset.rename_column(\"hate_type\", \"labels\")\n",
    "eval_dataset = eval_dataset.rename_column(\"hate_type\", \"labels\")\n",
    "\n",
    "# Initialize Trainer (same as before)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 711,
     "referenced_widgets": [
      "8711568fd4f24d34a91ccd5c0eb61dab",
      "002d1d448fd642d0a12c06ff02148cf0",
      "92cdb9bc53b74f3abf7443ac642d0e82",
      "3c240724353d4585a4090dbbff4f896e",
      "2ab20217c7c64114b23ceb561a8fb16c",
      "2e0312b6c73a438d99de69bab7f998ab",
      "53c85199e36340afa35f9884fa9b3d32",
      "b28d8ce411b3413e8cd55eee65b60d11",
      "de386b71578c4c0192ef9c324624adca",
      "90be4bb1013b4d41b5510e7dfbbf702f",
      "f09ac6c80dc7483aa6cea39890064097",
      "65e13271e58b4a63bb7bba86442c660a",
      "a3d898907b944083b21feef7c959201a",
      "412fd2585d074c9686479d26469095e4",
      "0c8d7c884eaa42a78a728fe6785b4550",
      "55e006a37c334d4b8fea8103a0b938d0",
      "02149fcbd98144b1979f4591d9f6cecd",
      "8f795334cbfd4085a1d3746986f433cb",
      "e8657f8c011546d0b9bda04f6f6bb0a9",
      "8cec2dae4f664a4e927430a3e117a5a5",
      "6a0533eff9044356845ae91fc378e483",
      "abdec0bce904449dbd23f8de13558e82"
     ]
    },
    "id": "hT4ubSiJKCz3",
    "outputId": "c3ebbb43-da42-407f-b8d6-898b8eee3933"
   },
   "outputs": [],
   "source": [
    "# Filter out samples with missing labels (-1) before training\n",
    "print(\"Filtering out samples with missing labels...\")\n",
    "\n",
    "# Filter training dataset\n",
    "print(f\"Original train dataset size: {len(train_dataset)}\")\n",
    "train_dataset = train_dataset.filter(lambda x: x['labels'] != -1)\n",
    "print(f\"Filtered train dataset size: {len(train_dataset)}\")\n",
    "\n",
    "# Filter validation dataset\n",
    "print(f\"Original eval dataset size: {len(eval_dataset)}\")\n",
    "eval_dataset = eval_dataset.filter(lambda x: x['labels'] != -1)\n",
    "print(f\"Filtered eval dataset size: {len(eval_dataset)}\")\n",
    "\n",
    "# Now train the model\n",
    "train_result = trainer.train()\n",
    "metrics = train_result.metrics\n",
    "max_train_samples = (\n",
    "    max_train_samples if 'max_train_samples' in locals() and max_train_samples is not None else len(train_dataset)\n",
    ")\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "# Saving the model and metrics\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n",
    "\n",
    "# Evaluating on validation data\n",
    "logger.info(\"*** Evaluate ***\")\n",
    "metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "max_eval_samples = (\n",
    "    max_eval_samples if 'max_eval_samples' in locals() and max_eval_samples is not None else len(eval_dataset)\n",
    ")\n",
    "metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "# Predicting the test data\n",
    "logger.info(\"*** Predict ***\")\n",
    "ids = predict_dataset['id']\n",
    "predict_dataset = predict_dataset.remove_columns(\"id\")\n",
    "predictions = trainer.predict(predict_dataset, metric_key_prefix=\"predict\").predictions\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Create output file\n",
    "output_predict_file = os.path.join(training_args.output_dir, f\"hate_type_predictions.tsv\")\n",
    "if trainer.is_world_process_zero():\n",
    "    with open(output_predict_file, \"w\") as writer:\n",
    "        logger.info(f\"***** Predict results *****\")\n",
    "        writer.write(\"id\\thate_type\\tmodel\\n\")\n",
    "        for index, item in enumerate(predictions):\n",
    "            # Convert prediction to original label\n",
    "            hate_type_label = id2hate_type[item]\n",
    "            writer.write(f\"{ids[index]}\\t{hate_type_label}\\t{model_name}\\n\")\n",
    "\n",
    "print(f\"First ID: {ids[0]}\")\n",
    "\n",
    "# Saving the model card\n",
    "kwargs = {\"finetuned_from\": model_name, \"tasks\": \"text-classification\"}\n",
    "trainer.create_model_card(**kwargs)\n",
    "\n",
    "# Create zip file\n",
    "!zip hate_predictions.zip {output_predict_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DorTufEETQOl"
   },
   "outputs": [],
   "source": [
    "### Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NmDIpSPSTeLw"
   },
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0wJygYVTjGE"
   },
   "outputs": [],
   "source": [
    "### Custom compute_metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = eval_pred.predictions\n",
    "    label_ids = eval_pred.label_ids\n",
    "\n",
    "    # Calculate accuracy for each task\n",
    "    hate_type_preds = np.argmax(predictions[0], axis=1)\n",
    "    severity_preds = np.argmax(predictions[1], axis=1)\n",
    "    to_whom_preds = np.argmax(predictions[2], axis=1)\n",
    "\n",
    "    hate_type_acc = accuracy_score(label_ids[0], hate_type_preds)\n",
    "    severity_acc = accuracy_score(label_ids[1], severity_preds)\n",
    "    to_whom_acc = accuracy_score(label_ids[2], to_whom_preds)\n",
    "\n",
    "    # Macro average accuracy\n",
    "    avg_acc = (hate_type_acc + severity_acc + to_whom_acc) / 3\n",
    "\n",
    "    return {\n",
    "        'hate_type_accuracy': hate_type_acc,\n",
    "        'severity_accuracy': severity_acc,\n",
    "        'to_whom_accuracy': to_whom_acc,\n",
    "        'average_accuracy': avg_acc\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNgj7a4O0q4DXbq/2kFEDmj",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
