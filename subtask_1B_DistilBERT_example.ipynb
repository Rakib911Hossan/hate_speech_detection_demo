{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# [Hate Speech Identification Shared Task](https://multihate.github.io/): Subtask 1B at [BLP Workshop](https://blp-workshop.github.io/) @IJCNLP-AACL 2025\n",
        "\n",
        "This shared task is designed to identify the type of hate, its severity, and the targeted group from social media content. The goal is to develop robust systems that advance research in this area.\n",
        "\n",
        "In this subtask, given a Bangla text collected from YouTube comments, categorize whether the hate towards individuals, organizations, communities, or society."
      ],
      "metadata": {
        "id": "Noik9q9c7Bhm",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading dataset from github"
      ],
      "metadata": {
        "id": "KSxBhCps7oBf",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvwQNYHk6kV5",
        "outputId": "447502f1-3fc3-41aa-9963-402bb45c8cfe",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-11 07:21:41--  https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1B/blp25_hatespeech_subtask_1B_train.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8005567 (7.6M) [text/plain]\n",
            "Saving to: ‘blp25_hatespeech_subtask_1B_train.tsv’\n",
            "\n",
            "blp25_hatespeech_su 100%[===================>]   7.63M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-07-11 07:21:41 (91.6 MB/s) - ‘blp25_hatespeech_subtask_1B_train.tsv’ saved [8005567/8005567]\n",
            "\n",
            "--2025-07-11 07:21:41--  https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1B/blp25_hatespeech_subtask_1B_dev.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 585702 (572K) [text/plain]\n",
            "Saving to: ‘blp25_hatespeech_subtask_1B_dev.tsv’\n",
            "\n",
            "blp25_hatespeech_su 100%[===================>] 571.97K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-07-11 07:21:41 (15.8 MB/s) - ‘blp25_hatespeech_subtask_1B_dev.tsv’ saved [585702/585702]\n",
            "\n",
            "--2025-07-11 07:21:42--  https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1B/blp25_hatespeech_subtask_1B_dev_test.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 548258 (535K) [text/plain]\n",
            "Saving to: ‘blp25_hatespeech_subtask_1B_dev_test.tsv’\n",
            "\n",
            "blp25_hatespeech_su 100%[===================>] 535.41K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-07-11 07:21:42 (14.9 MB/s) - ‘blp25_hatespeech_subtask_1B_dev_test.tsv’ saved [548258/548258]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1B/blp25_hatespeech_subtask_1B_train.tsv\n",
        "!wget https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1B/blp25_hatespeech_subtask_1B_dev.tsv\n",
        "!wget https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1B/blp25_hatespeech_subtask_1B_dev_test.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### installing required libraries.\n",
        " - transformers\n",
        " - datasets\n",
        " - evaluate\n",
        " - accelerate"
      ],
      "metadata": {
        "id": "xYZ96DWt-TZk",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "# !pip install --upgrade accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLJh5GGU-xET",
        "outputId": "f6bd00bc-68ed-43a1-ea73-8a5839d7e663",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### importing required libraries and setting up logger"
      ],
      "metadata": {
        "id": "OXhVWUJ3A_hx",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "import pandas as pd\n",
        "import datasets\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "import torch\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorWithPadding,\n",
        "    EvalPrediction,\n",
        "    HfArgumentParser,\n",
        "    PretrainedConfig,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    default_data_collator,\n",
        "    set_seed,\n",
        ")\n",
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "from transformers.utils import check_min_version, send_example_telemetry\n",
        "from transformers.utils.versions import require_version\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    handlers=[logging.StreamHandler(sys.stdout)],\n",
        ")"
      ],
      "metadata": {
        "id": "VIUAU0rRBOmR",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the training, validation, and test data"
      ],
      "metadata": {
        "id": "HP6CdL7NHpxJ",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_file = 'blp25_hatespeech_subtask_1B_train.tsv'\n",
        "validation_file = 'blp25_hatespeech_subtask_1B_dev.tsv'\n",
        "test_file = 'blp25_hatespeech_subtask_1B_dev_test.tsv'"
      ],
      "metadata": {
        "id": "bMzfE34iHyGV",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Disable wandb"
      ],
      "metadata": {
        "id": "w59H3fOnLUcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "CRQSQF6MLYrB"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the training parameters"
      ],
      "metadata": {
        "id": "3-_w4YehCgX4",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    output_dir=\"./distilBERT_m/\",\n",
        "    overwrite_output_dir=True,\n",
        "    remove_unused_columns=False,\n",
        "    local_rank= 1,\n",
        "    load_best_model_at_end=True,\n",
        "    save_total_limit=2,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=None\n",
        ")\n",
        "\n",
        "max_train_samples = None\n",
        "max_eval_samples=None\n",
        "max_predict_samples=None\n",
        "max_seq_length = 512\n",
        "batch_size = 16"
      ],
      "metadata": {
        "id": "7-GUUNj0BPbu",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffb5cbd8-2121-44d8-95df-b2de094f2bd9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|training_args.py:2186] 2025-07-11 07:25:11,523 >> PyTorch: setting up devices\n",
            "[INFO|training_args.py:1863] 2025-07-11 07:25:11,542 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "[WARNING|integration_utils.py:101] 2025-07-11 07:25:11,544 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformers.utils.logging.set_verbosity_info()\n",
        "\n",
        "log_level = training_args.get_process_log_level()\n",
        "logger.setLevel(log_level)\n",
        "datasets.utils.logging.set_verbosity(log_level)\n",
        "transformers.utils.logging.set_verbosity(log_level)\n",
        "transformers.utils.logging.enable_default_handler()\n",
        "transformers.utils.logging.enable_explicit_format()\n",
        "logger.warning(\n",
        "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
        "    + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
        ")\n",
        "logger.info(f\"Training/evaluation parameters {training_args}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Q4deAnUJ0iI",
        "outputId": "3b211d54-3b97-4a89-dff9-92b751200afd",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=IntervalStrategy.NO,\n",
            "eval_use_gather_object=False,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=False,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_revision=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "liger_kernel_config=None,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./distilBERT_m/runs/Jul11_07-23-11_28adcce3f1e4,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=loss,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1,\n",
            "optim=OptimizerNames.ADAMW_TORCH,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=./distilBERT_m/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=16,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=False,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./distilBERT_m/,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=SaveStrategy.NO,\n",
            "save_total_limit=2,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Defining the Model"
      ],
      "metadata": {
        "id": "RgkvwlbFHVo5",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'distilbert-base-multilingual-cased'"
      ],
      "metadata": {
        "id": "-De1tz5qHYre",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### setting the random seed"
      ],
      "metadata": {
        "id": "yPqrrDbcKN8n",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(training_args.seed)"
      ],
      "metadata": {
        "id": "ZvKpoxaQKTB6",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading data files"
      ],
      "metadata": {
        "id": "bgNrs7AhKdvl",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l2id = {'None': 0, 'Society': 1, 'Organization': 2, 'Community': 3, 'Individual': 4}\n",
        "train_df = pd.read_csv(train_file, sep='\\t')\n",
        "# print(train_df['label'])\n",
        "train_df['label'] = train_df['label'].map(l2id).fillna(0).astype(int)\n",
        "train_df = Dataset.from_pandas(train_df)\n",
        "validation_df = pd.read_csv(validation_file, sep='\\t')\n",
        "validation_df['label'] = validation_df['label'].map(l2id).fillna(0).astype(int)\n",
        "validation_df = Dataset.from_pandas(validation_df)\n",
        "test_df = pd.read_csv(test_file, sep='\\t')\n",
        "#test_df['label'] = test_df['label'].map(l2id)\n",
        "test_df = Dataset.from_pandas(test_df)\n",
        "\n",
        "data_files = {\"train\": train_df, \"validation\": validation_df, \"test\": test_df}\n",
        "for key in data_files.keys():\n",
        "    logger.info(f\"loading a local file for {key}\")\n",
        "raw_datasets = DatasetDict(\n",
        "    {\"train\": train_df, \"validation\": validation_df, \"test\": test_df}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDwaW8AnKcgD",
        "outputId": "190a1a9f-9714-408b-bcd4-f860f836004b",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:loading a local file for train\n",
            "INFO:__main__:loading a local file for validation\n",
            "INFO:__main__:loading a local file for test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_df['id'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1swECfaTuJl",
        "outputId": "7b5643cd-99e1-4b59-f6c2-66cd4866b240"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2512"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Extracting number of unique labels"
      ],
      "metadata": {
        "id": "BJhNu7tPQ2RU",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Labels\n",
        "label_list = raw_datasets[\"train\"].unique(\"label\")\n",
        "print(label_list)\n",
        "label_list.sort()  # sort the labels for determine\n",
        "num_labels = len(label_list)"
      ],
      "metadata": {
        "id": "JTl6NNPmOXhO",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51e6ab13-27bf-4490-edb3-58dd650854eb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 4, 1, 3, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Pretrained Configuration, Tokenizer and Model"
      ],
      "metadata": {
        "id": "J1dpoOAPRJnN",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = AutoConfig.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_labels,\n",
        "    finetuning_task=None,\n",
        "    cache_dir=None,\n",
        "    revision=\"main\",\n",
        "    use_auth_token=None,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    cache_dir=None,\n",
        "    use_fast=True,\n",
        "    revision=\"main\",\n",
        "    use_auth_token=None,\n",
        ")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    from_tf=bool(\".ckpt\" in model_name),\n",
        "    config=config,\n",
        "    cache_dir=None,\n",
        "    revision=\"main\",\n",
        "    use_auth_token=None,\n",
        "    ignore_mismatched_sizes=False,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2a367cacc8e244eca07204e5a1647327",
            "5fa63b1d202e4f7fa6d497ebcddf6be3",
            "9b936b66609b4a1db265a547415f2d7f",
            "d595a59d9ccf466981ad948f5f921345",
            "999ad3a10635443886f71957f4a950bc",
            "46e0b4c82fa5448f848d164335556aea",
            "5116a2bdd71e42e094dc536db5712c05",
            "3a53f881f9aa4e799c77bd4992ff59de",
            "ce5e81ba6913445eb60dc4917f1778f4",
            "0dee7d5c03304265919beea131d1c5c8",
            "436c7f44b17f43f58fd40cbf95115cb4",
            "9e4c3c5eb7d74cc79b77b022b1452c99",
            "92fd9a52f460421991342a2d3c7e5fbb",
            "0d2e33660a5a46c4b023f6e156f0a211",
            "80fd84e8643c401aac76389917d1cd48",
            "8531465b19634de787938c583b784282",
            "7b8a6f63de864ddda7171a7329ad18c6",
            "12d7a6ad603a40309eef9041ed8f4a04",
            "24e055f5251847179e5f93a5610c1206",
            "9fdd5aac0ff249ac8c4810cb83ea2cc4",
            "84eac023163d46218e73cc8a13de1539",
            "e0dd9275fd734f419791435b7a46cdab",
            "0b3613a4d4e849fc849cc07f4e8582c3",
            "42f6a05689bc46f4813d455d24056011",
            "f6e8fc7eaf694b038b1e122b4ddd4536",
            "d15425caa34a4590a7f3e38857163ee2",
            "ed7be05dcc9949338204d6ab518fb2ad",
            "4303c4ca6bf749a6a91b5a161f688865",
            "f313d24f8a3448b9b628056785e6f209",
            "59416b9fad2b400e9540e2c678e42ba6",
            "3e4b315b19644420b16deed5187f25f4",
            "29086ae0d34a4dc4b414706e23f6661d",
            "2c59206449024fa68de305b3b2c38c0a",
            "3ff5affc09ae451199d50c0829229ede",
            "52eb3aa1d4484b62b8693e5d6f7fb2a5",
            "64bce23820664926a768a675f616d8e1",
            "4a0ce98f22ab470282a28eff42571152",
            "3bacb25d28e546ada9e8303a765d5f61",
            "1bde985270af4b0f80916351eea69289",
            "69a04f85e9334ee2bcfb0c043cd729ef",
            "5309a7d246e242e1bc506a2d81a6a255",
            "716b0ddf017c4c23a316cfa48ae53443",
            "491d9c31d8cb40f5952653475916c0da",
            "5055cad5968d40e8a4dad427d24d9d47",
            "c1fbc4c2734440d088495be489adc462",
            "fd504c5b5e3d49a0a2245fc43d56aaec",
            "dbca9d47b9ad4e9ab9f56c6d5d773bb9",
            "c9b660fc4cb9430d9d9e04caa6bb804f",
            "7cf1fb8ae46b4ccabe304d1475d96820",
            "97ca98508f78498faa5b69edf565841c",
            "b14afe22d6a94311b822d2707246408e",
            "bb848827148e4838a2250c7f5bf3e7fb",
            "1ceb9026519846c0b687390bb57d055c",
            "8d380155d03f445ea75cacfa85967f9f",
            "b76a38633e504a568c82a78c0cd9ec83"
          ]
        },
        "id": "jmAaMuBuRQd2",
        "outputId": "425a11e9-40da-45a8-dc9a-45c0a2d8c8e7",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a367cacc8e244eca07204e5a1647327"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|configuration_utils.py:711] 2025-07-11 07:23:48,079 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-multilingual-cased/snapshots/45c032ab32cc946ad88a166f7cb282f58c753c2e/config.json\n",
            "[INFO|configuration_utils.py:774] 2025-07-11 07:23:48,137 >> Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4\n",
            "  },\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.53.1\",\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e4c3c5eb7d74cc79b77b022b1452c99"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|configuration_utils.py:711] 2025-07-11 07:23:48,384 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-multilingual-cased/snapshots/45c032ab32cc946ad88a166f7cb282f58c753c2e/config.json\n",
            "[INFO|configuration_utils.py:774] 2025-07-11 07:23:48,387 >> Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.53.1\",\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b3613a4d4e849fc849cc07f4e8582c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ff5affc09ae451199d50c0829229ede"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|tokenization_utils_base.py:2012] 2025-07-11 07:23:49,623 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-multilingual-cased/snapshots/45c032ab32cc946ad88a166f7cb282f58c753c2e/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2012] 2025-07-11 07:23:49,624 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-multilingual-cased/snapshots/45c032ab32cc946ad88a166f7cb282f58c753c2e/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2012] 2025-07-11 07:23:49,625 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2012] 2025-07-11 07:23:49,625 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2012] 2025-07-11 07:23:49,626 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-multilingual-cased/snapshots/45c032ab32cc946ad88a166f7cb282f58c753c2e/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2012] 2025-07-11 07:23:49,627 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|configuration_utils.py:711] 2025-07-11 07:23:49,628 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-multilingual-cased/snapshots/45c032ab32cc946ad88a166f7cb282f58c753c2e/config.json\n",
            "[INFO|configuration_utils.py:774] 2025-07-11 07:23:49,629 >> Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.53.1\",\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/542M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1fbc4c2734440d088495be489adc462"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|modeling_utils.py:1267] 2025-07-11 07:24:06,221 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-multilingual-cased/snapshots/45c032ab32cc946ad88a166f7cb282f58c753c2e/model.safetensors\n",
            "[INFO|modeling_utils.py:5374] 2025-07-11 07:24:06,465 >> Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:5386] 2025-07-11 07:24:06,478 >> Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocessing the raw_datasets"
      ],
      "metadata": {
        "id": "m7PIQVypeTf4",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "non_label_column_names = [name for name in raw_datasets[\"train\"].column_names if name != \"label\"]\n",
        "sentence1_key= non_label_column_names[1]\n",
        "\n",
        "# Padding strategy\n",
        "padding = \"max_length\"\n",
        "\n",
        "# Some models have set the order of the labels to use, so let's make sure we do use it.\n",
        "label_to_id = None\n",
        "if (model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id):\n",
        "    # Some have all caps in their config, some don't.\n",
        "    label_name_to_id = {k.lower(): v for k, v in model.config.label2id.items()}\n",
        "    if sorted(label_name_to_id.keys()) == sorted(label_list):\n",
        "        label_to_id = {i: int(label_name_to_id[label_list[i]]) for i in range(num_labels)}\n",
        "    else:\n",
        "        logger.warning(\n",
        "            \"Your model seems to have been trained with labels, but they don't match the dataset: \",\n",
        "            f\"model labels: {sorted(label_name_to_id.keys())}, dataset labels: {sorted(label_list)}.\"\n",
        "            \"\\nIgnoring the model labels as a result.\",)\n",
        "\n",
        "if label_to_id is not None:\n",
        "    model.config.label2id = label_to_id\n",
        "    model.config.id2label = {id: label for label, id in config.label2id.items()}\n",
        "\n",
        "if 128 > tokenizer.model_max_length:\n",
        "    logger.warning(\n",
        "        f\"The max_seq_length passed ({128}) is larger than the maximum length for the\"\n",
        "        f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\")\n",
        "max_seq_length = min(128, tokenizer.model_max_length)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # Tokenize the texts\n",
        "    args = (\n",
        "        (examples[sentence1_key],))\n",
        "    result = tokenizer(*args, padding=padding, max_length=max_seq_length, truncation=True)\n",
        "\n",
        "    # Map labels to IDs (not necessary for GLUE tasks)\n",
        "    if label_to_id is not None and \"label\" in examples:\n",
        "        result[\"label\"] = [(label_to_id[l] if l != -1 else -1) for l in examples[\"label\"]]\n",
        "    return result\n",
        "raw_datasets = raw_datasets.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    load_from_cache_file=True,\n",
        "    desc=\"Running tokenizer on dataset\",\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "4693545502524c2e87dc47f20e0f01f5",
            "a6e8ff077fe64ba3a0fc8af70a31c16f",
            "8200c085e0f3419eb30dc15e722222f2",
            "0bbdd47ccce04bb485278c27f8c7ffbf",
            "703ed98e91c9408a8f657346a69b7a45",
            "c91a2696e18344bc9d1bab35f9e19325",
            "4a6f8746d6ed4648a77b0aca904fc3fd",
            "bafdb97957074fd4a0ef98d23c4d2864",
            "bb0bfa18a6b44215aeb3d697a324ba02",
            "458953e5d11d428daef475f18d4ba873",
            "3fc0f5de84794c4aa327f7db01c6914b",
            "a137785622034127b8e069dccb2e59b7",
            "e8f98dadfd954bae91b71801800af131",
            "2415521496c2441ab7fd54231f258b18",
            "e08526ca5720438eb2fe2621617911b0",
            "76b6b511feea4dad99bffa00c44d0f9b",
            "a000ca0158f1401c86cf03c7db65d363",
            "47f90a5d817748018962efdb1acdfb1e",
            "e0277e8bd0f74ef2a1300ab50fef65a8",
            "0866337f1c9549a5b9fd7b9b4dbb738d",
            "51c32e88db0e48f1b5190b10ba13c6af",
            "542e6f476d6b48f388ae6f8987fb4b3d",
            "6b6a04470c13464686bb2047c699a41e",
            "eb4e8622f50d4930a38ba1475092bd13",
            "02f77d7f9bde474bac73372faeaa309a",
            "254b948d3b76467e99f7f704b585eab8",
            "e387e9c79e41459781cc88b2eae4973e",
            "e6b9e5717d694036be683bd66ecebe9e",
            "e18eef4e065a457183d0f01f2a56d789",
            "36faeccf793546e8a4773b8337439352",
            "e9af982254f243ec9ea223bdac13a334",
            "bfa03284f4474a059e1263aab189f88f",
            "ef627e669d9d4f4ca3fa80b73c967eea"
          ]
        },
        "id": "pqO3YWAZelhd",
        "outputId": "adea08f8-f611-4839-a3bd-1c1119b842d2",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running tokenizer on dataset:   0%|          | 0/35522 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4693545502524c2e87dc47f20e0f01f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running tokenizer on dataset:   0%|          | 0/2512 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a137785622034127b8e069dccb2e59b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running tokenizer on dataset:   0%|          | 0/2512 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b6a04470c13464686bb2047c699a41e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Finalize the training data for training the model"
      ],
      "metadata": {
        "id": "ASxWKiqifb_g",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if \"train\" not in raw_datasets:\n",
        "    raise ValueError(\"requires a train dataset\")\n",
        "train_dataset = raw_datasets[\"train\"]\n",
        "if max_train_samples is not None:\n",
        "    max_train_samples_n = min(len(train_dataset), max_train_samples)\n",
        "    train_dataset = train_dataset.select(range(max_train_samples_n))"
      ],
      "metadata": {
        "id": "QHoDqrBGgD6F",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqME25nm-hwo",
        "outputId": "bae519b8-502e-49a0-8dfa-1307a4fb4c82",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'text', 'label', 'input_ids', 'attention_mask'],\n",
              "    num_rows: 35522\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Finalize the development/evaluation data for evaluating the model"
      ],
      "metadata": {
        "id": "k72vUTSigOzZ",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if \"validation\" not in raw_datasets:\n",
        "    raise ValueError(\"requires a validation dataset\")\n",
        "eval_dataset = raw_datasets[\"validation\"]\n",
        "if max_eval_samples is not None:\n",
        "    max_eval_samples_n = min(len(eval_dataset), max_eval_samples)\n",
        "    eval_dataset = eval_dataset.select(range(max_eval_samples_n))"
      ],
      "metadata": {
        "id": "MqrW8ospgUYZ",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Finalize the test data for predicting the unseen test data using the model"
      ],
      "metadata": {
        "id": "B7sVqp3hgU4i",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if \"test\" not in raw_datasets and \"test_matched\" not in raw_datasets:\n",
        "    raise ValueError(\"requires a test dataset\")\n",
        "predict_dataset = raw_datasets[\"test\"]\n",
        "if max_predict_samples is not None:\n",
        "    max_predict_samples_n = min(len(predict_dataset), max_predict_samples)\n",
        "    predict_dataset = predict_dataset.select(range(max_predict_samples_n))"
      ],
      "metadata": {
        "id": "u0dBjIQggcYs",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Log a few random samples from the training set"
      ],
      "metadata": {
        "id": "Cqbo1xzRge36",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index in random.sample(range(len(train_dataset)), 3):\n",
        "    logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIO2bxSVgkLb",
        "outputId": "2435eaec-b822-44ad-b3a6-8e6a8ee63055",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Sample 7296 of the training set: {'id': 660, 'text': 'সরকারের দায়িত্বপ্রাপ্ত সংস্থা সকল বাণিজ্যিক ভবন গুলোকে বছরে অন্তত একবার পরিদর্শন করে রিপোর্ট প্রদান করবেন একটা বাণিজ্যিক ভবনে নিরাপত্তার সকল ব্যবস্থা থাকা উচিত কিন্তু এই সব সম্ভবের দেশে কিছুই করা হয় না এই দেশে মানুষের জীবনের কোন দাম নেই তাই কেউ কোনকিছুর তদারকিরও দরকার মনে করে না', 'label': 0, 'input_ids': [101, 62853, 11421, 100, 978, 22756, 102069, 87664, 17660, 20513, 80198, 15215, 15691, 971, 75762, 950, 30277, 16431, 18243, 32465, 11199, 937, 70115, 13542, 28777, 36213, 29740, 12235, 17511, 11128, 111240, 40433, 13334, 974, 12235, 22335, 47719, 33072, 76677, 948, 11128, 49178, 11737, 28777, 36715, 17660, 20513, 80198, 15215, 15691, 971, 75762, 11199, 967, 27268, 96397, 13542, 72659, 11128, 87664, 970, 15215, 106676, 964, 81661, 66449, 17460, 35003, 14112, 100024, 978, 15002, 111240, 61596, 100047, 88324, 11199, 37855, 14998, 14704, 100, 26109, 14112, 88324, 11199, 109216, 955, 13100, 75762, 11421, 41431, 965, 58354, 967, 59712, 52038, 14998, 82937, 89362, 41431, 70295, 53574, 29261, 963, 77960, 70295, 11128, 18262, 965, 11128, 35884, 84851, 13334, 26109, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 1639 of the training set: {'id': 463531, 'text': 'কে কে টা রোজ রাখবো এবং পাঁচ ওয়াক্ত নামাজ আদায় করব', 'label': 0, 'input_ids': [101, 82937, 82937, 958, 12079, 974, 16431, 24383, 974, 12079, 42651, 19910, 16431, 12051, 968, 12079, 111219, 39427, 100, 35059, 100277, 100, 948, 11128, 19910, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 18024 of the training set: {'id': 156766, 'text': 'উনাদের উচ্ছেদ করা উচিত হবেনা উনারা আমাদের জাতি সত্তার অংশ', 'label': 0, 'input_ids': [101, 941, 20979, 16755, 66449, 111240, 32294, 17511, 14704, 66449, 17460, 53761, 20979, 941, 71416, 12079, 938, 55631, 16755, 955, 43004, 12235, 978, 13542, 72659, 11128, 47687, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Get the metric function `accuracy`"
      ],
      "metadata": {
        "id": "nAcn0Pc8gogF",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"accuracy\")"
      ],
      "metadata": {
        "id": "aMWMQdaUgvAq",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1f0018d516854e828290c94aaef4113c",
            "db2eef92804040d89feccdf3981e6f1d",
            "9eff91e8c473407ba3140ebe2c204168",
            "35f0d3ef8d8141b8810654f49ce55546",
            "4f92e18fd6694fb7aa060bb7212245b1",
            "f9709aebfc0a4b24a38b8b24b2d0f3f3",
            "46b4a0d75ff94a0f8ecbb7535f35fe51",
            "e81a3a14437c45509709d04d3d84da8f",
            "701f694850e547c58f6a25799a327fdb",
            "1d55e032f67d4d95915ec433c37e884b",
            "b65f654ef0a843eca9dda6e1190a93d3"
          ]
        },
        "outputId": "2ee1300f-c209-481d-db12-a7d4a14a7ec4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f0018d516854e828290c94aaef4113c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predictions and label_ids field and has to return a dictionary string to float."
      ],
      "metadata": {
        "id": "foWUyuBHgxbA",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
        "    preds = np.argmax(preds, axis=1)\n",
        "    return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\n"
      ],
      "metadata": {
        "id": "-3VqxkqcgxCC",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Collator"
      ],
      "metadata": {
        "id": "dNWK1Hfbg8-o",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = default_data_collator"
      ],
      "metadata": {
        "id": "_w6lNh-OhJLC",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initialize our Trainer"
      ],
      "metadata": {
        "id": "2nYlugPRhNbg",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.remove_columns(\"id\")\n",
        "eval_dataset = eval_dataset.remove_columns(\"id\")"
      ],
      "metadata": {
        "id": "i-rwWO7wOpok"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "yeJco0JOhPHx",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4982d72b-1bd9-4c30-edba-759f64e32571"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-32-1049306949.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training our model"
      ],
      "metadata": {
        "id": "cUxWn9HrhqRM",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_result = trainer.train()\n",
        "metrics = train_result.metrics\n",
        "max_train_samples = (\n",
        "    max_train_samples if max_train_samples is not None else len(train_dataset)\n",
        ")\n",
        "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "B681qnPFhtY0",
        "outputId": "69195940-849c-4903-8b7e-0f7b9cd5aff2",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|trainer.py:2402] 2025-07-11 07:25:21,245 >> ***** Running training *****\n",
            "[INFO|trainer.py:2403] 2025-07-11 07:25:21,245 >>   Num examples = 35,522\n",
            "[INFO|trainer.py:2404] 2025-07-11 07:25:21,246 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:2405] 2025-07-11 07:25:21,247 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:2408] 2025-07-11 07:25:21,248 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:2409] 2025-07-11 07:25:21,248 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2410] 2025-07-11 07:25:21,249 >>   Total optimization steps = 2,221\n",
            "[INFO|trainer.py:2411] 2025-07-11 07:25:21,250 >>   Number of trainable parameters = 135,328,517\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2221' max='2221' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2221/2221 07:30, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.083500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.966200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.912700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.883300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|trainer.py:2677] 2025-07-11 07:32:53,041 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Saving the tokenizer too for easy upload"
      ],
      "metadata": {
        "id": "SaaRglkwllSp",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model()\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "trainer.save_state()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UwoMEbAloMx",
        "outputId": "1a9141b8-1d25-47b1-ec34-2451efcabcca",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|trainer.py:4019] 2025-07-11 07:32:53,096 >> Saving model checkpoint to ./distilBERT_m/\n",
            "[INFO|configuration_utils.py:437] 2025-07-11 07:32:53,099 >> Configuration saved in ./distilBERT_m/config.json\n",
            "[INFO|modeling_utils.py:3949] 2025-07-11 07:33:01,067 >> Model weights saved in ./distilBERT_m/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2507] 2025-07-11 07:33:01,072 >> tokenizer config file saved in ./distilBERT_m/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2516] 2025-07-11 07:33:01,073 >> Special tokens file saved in ./distilBERT_m/special_tokens_map.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  total_flos               =  1095644GF\n",
            "  train_loss               =     0.9517\n",
            "  train_runtime            = 0:07:31.83\n",
            "  train_samples            =      35522\n",
            "  train_samples_per_second =     78.618\n",
            "  train_steps_per_second   =      4.916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluating our model on validation/development data"
      ],
      "metadata": {
        "id": "K9zCKBGEhwb7",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"*** Evaluate ***\")\n",
        "\n",
        "metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
        "\n",
        "max_eval_samples = (\n",
        "    max_eval_samples if max_eval_samples is not None else len(eval_dataset)\n",
        ")\n",
        "metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
        "\n",
        "trainer.log_metrics(\"eval\", metrics)\n",
        "trainer.save_metrics(\"eval\", metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "YClw3dXTh17u",
        "outputId": "65b6484e-be86-47a9-e396-31c01e516404",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:4353] 2025-07-11 07:33:01,178 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4355] 2025-07-11 07:33:01,178 >>   Num examples = 2512\n",
            "[INFO|trainer.py:4358] 2025-07-11 07:33:01,179 >>   Batch size = 16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [157/157 00:08]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** eval metrics *****\n",
            "  epoch                   =        1.0\n",
            "  eval_accuracy           =     0.6939\n",
            "  eval_loss               =     0.8355\n",
            "  eval_runtime            = 0:00:08.92\n",
            "  eval_samples            =       2512\n",
            "  eval_samples_per_second =    281.499\n",
            "  eval_steps_per_second   =     17.594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predecting the test data"
      ],
      "metadata": {
        "id": "Y3LSdUdPh7uG",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id2l = {v: k for k, v in l2id.items()}\n",
        "logger.info(\"*** Predict ***\")\n",
        "#predict_dataset = predict_dataset.remove_columns(\"label\")\n",
        "ids = predict_dataset['id']\n",
        "predict_dataset = predict_dataset.remove_columns(\"id\")\n",
        "predictions = trainer.predict(predict_dataset, metric_key_prefix=\"predict\").predictions\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "output_predict_file = os.path.join(training_args.output_dir, f\"subtask_1B.tsv\")\n",
        "if trainer.is_world_process_zero():\n",
        "    with open(output_predict_file, \"w\") as writer:\n",
        "        logger.info(f\"***** Predict results *****\")\n",
        "        writer.write(\"id\\tlabel\\tmodel\\n\")\n",
        "        for index, item in enumerate(predictions):\n",
        "            item = label_list[item]\n",
        "            item = id2l[item]\n",
        "            writer.write(f\"{ids[index]}\\t{item}\\t{model_name}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "gnXhVq6Yh_oS",
        "outputId": "8ace4ed3-2abe-470d-f2c4-32e1dadeb7fc",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:*** Predict ***\n",
            "[INFO|trainer.py:4353] 2025-07-11 07:33:10,551 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:4355] 2025-07-11 07:33:10,551 >>   Num examples = 2512\n",
            "[INFO|trainer.py:4358] 2025-07-11 07:33:10,552 >>   Batch size = 16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:***** Predict results *****\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Gqqk_24__47",
        "outputId": "cc468399-5a17-4965-c4a4-ccf021a67ba0",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "879187"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Saving the model into card"
      ],
      "metadata": {
        "id": "fQgoTTIoiI0X",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kwargs = {\"finetuned_from\": model_name, \"tasks\": \"text-classification\"}\n",
        "trainer.create_model_card(**kwargs)"
      ],
      "metadata": {
        "id": "B1ooJgrViLVj",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1be4029d-def8-444e-a711-d1e3ac2679c8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|modelcard.py:450] 2025-07-11 07:33:19,898 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.693869411945343}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip subtask_1B.zip ./distilBERT_m/subtask_1B.tsv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2Uq2f0CQXvu",
        "outputId": "9f8d7d31-e745-4d4d-dbc2-dfd410c4a5dd"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: distilBERT_m/subtask_1B.tsv (deflated 90%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yNFwIyDfQ4Sl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
