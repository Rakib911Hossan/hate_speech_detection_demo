{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rakib911Hossan/hate_speech_detection_demo/blob/main/subtask_1A_xlm_roberta_qwen_label_Final_Bangla_Bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-4l-MeZlwmLU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e0f1036-b73f-4c0a-efaa-1d53200b7f90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 35522 samples\n",
            "Validation: 2512 samples\n",
            "Test: 2512 samples\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the URLs for the datasets\n",
        "train_url = 'https://raw.githubusercontent.com/Rakib911Hossan/hate_speech_task_A_dataset_null_filled_with_qwen/main/train_converted.csv'\n",
        "val_url = 'https://raw.githubusercontent.com/Rakib911Hossan/hate_speech_task_A_dataset_null_filled_with_qwen/main/val_converted.csv'\n",
        "test_url = 'https://raw.githubusercontent.com/Rakib911Hossan/hate_speech_task_A_dataset_null_filled_with_qwen/main/test_converted.csv'\n",
        "\n",
        "# Load the datasets\n",
        "train_df = pd.read_csv(train_url)\n",
        "val_df = pd.read_csv(val_url)\n",
        "test_df = pd.read_csv(test_url)\n",
        "\n",
        "# Display the number of samples in each dataset\n",
        "print(f\"Train: {len(train_df)} samples\")\n",
        "print(f\"Validation: {len(val_df)} samples\")\n",
        "print(f\"Test: {len(test_df)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UGZN4ksxOGs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41b5e729-b66c-4c65-b902-2488112f6589"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.34.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.34.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install matplotlib seaborn scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RknymTYhxXfp"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "import pandas as pd\n",
        "import datasets\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "import re\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorWithPadding,\n",
        "    EvalPrediction,\n",
        "    HfArgumentParser,\n",
        "    PretrainedConfig,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    default_data_collator,\n",
        "    set_seed,\n",
        ")\n",
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "from transformers.utils import check_min_version, send_example_telemetry\n",
        "from transformers.utils.versions import require_version\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    handlers=[logging.StreamHandler(sys.stdout)],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-oxDfg5xaOa"
      },
      "outputs": [],
      "source": [
        "# Data types & non-null counts\n",
        "print(\"Train dataset info:\")\n",
        "print(train_df.info())\n",
        "\n",
        "# First few rows\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(train_df.head())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values in train dataset:\")\n",
        "print(train_df.isnull().sum())\n",
        "\n",
        "# Check label distribution\n",
        "if 'labels' in train_df.columns:\n",
        "    print(\"\\nLabel distribution:\")\n",
        "    print(train_df['labels'].value_counts(normalize=True))\n",
        "\n",
        "    train_df['labels'].value_counts().plot(kind='bar')\n",
        "    plt.title('Label Distribution')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGgnElRLxeCd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6_VCAscxoBI"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, EarlyStoppingCallback\n",
        "\n",
        "# üéØ OPTIMIZED CONFIGURATION\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./banglabert_hate_speech/\",\n",
        "    overwrite_output_dir=True,\n",
        "\n",
        "    # üîß ADJUSTED LEARNING SCHEDULE\n",
        "    learning_rate=10e-6,                     # Better for BERT models\n",
        "    num_train_epochs=8,                     # HateBERT converges faster\n",
        "    warmup_ratio=0.15,                       # Less warmup needed\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "\n",
        "    # üõ°Ô∏è STRONGER REGULARIZATION\n",
        "    weight_decay=0.01,                      # Weight decay\n",
        "    max_grad_norm=0.5,                      # Tighter gradient clipping\n",
        "    dataloader_drop_last=True,              # More consistent batch sizes\n",
        "\n",
        "    # ‚úÖ BATCH CONFIGURATION\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=1,\n",
        "\n",
        "    # üéØ EARLY STOPPING\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_micro_f1\",\n",
        "    greater_is_better=True,\n",
        "\n",
        "    # üìä MONITORING\n",
        "    logging_steps=250,\n",
        "    eval_steps=None,\n",
        "    save_total_limit=3,\n",
        "\n",
        "    # üîß SYSTEM OPTIMIZATIONS\n",
        "    report_to=None,\n",
        "    dataloader_num_workers=2,\n",
        "    fp16=True,\n",
        "    group_by_length=True,\n",
        ")\n",
        "\n",
        "# üõë EARLY STOPPING\n",
        "early_stopping = EarlyStoppingCallback(\n",
        "    early_stopping_patience=2,\n",
        "    early_stopping_threshold=0.001\n",
        ")\n",
        "\n",
        "# üéØ DATA PARAMETERS\n",
        "max_train_samples = None\n",
        "max_eval_samples = None\n",
        "max_predict_samples = None\n",
        "max_seq_length = 512\n",
        "batch_size = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEn-5lyXxvUM"
      },
      "outputs": [],
      "source": [
        "transformers.utils.logging.set_verbosity_info()\n",
        "\n",
        "log_level = training_args.get_process_log_level()\n",
        "logger.setLevel(log_level)\n",
        "datasets.utils.logging.set_verbosity(log_level)\n",
        "transformers.utils.logging.set_verbosity(log_level)\n",
        "transformers.utils.logging.enable_default_handler()\n",
        "transformers.utils.logging.enable_explicit_format()\n",
        "\n",
        "logger.warning(\n",
        "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
        "    + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
        ")\n",
        "logger.info(f\"Training/evaluation parameters {training_args}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KWgWVU9xzp-"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Model setup\n",
        "model_name = 'csebuetnlp/banglabert'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=6  # Your 5 classes\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8YX1WLbzK1k"
      },
      "outputs": [],
      "source": [
        "set_seed(training_args.seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qR0hNG09zSer"
      },
      "outputs": [],
      "source": [
        "# Create label mapping\n",
        "l2id = {'None': 0, 'Abusive': 1, 'Sexism': 2, 'Religious Hate': 3, 'Political Hate': 4, 'Profane': 5}\n",
        "id2l = {v: k for k, v in l2id.items()}\n",
        "\n",
        "class KeywordFeatureExtractor:\n",
        "    def __init__(self):\n",
        "        # Based on your chi-squared analysis\n",
        "        self.label_keywords = {\n",
        "            'Profane': ['‡¶¨‡¶æ‡¶≤', '‡¶Æ‡¶æ‡¶ó‡¶ø‡¶∞', '‡¶¨‡¶æ‡¶≤‡ßá‡¶∞', '‡¶ñ‡¶æ‡¶®‡¶ï‡¶ø‡¶∞', '‡¶¨‡ßá‡¶∂‡ßç‡¶Ø‡¶æ', '‡¶¶‡¶´‡¶æ', '‡¶¨‡¶æ‡¶ö‡ßç‡¶ö‡¶æ', '‡¶∏‡¶æ‡¶≤‡¶æ', '‡¶∂‡¶æ‡¶≤‡¶æ',\n",
        "                       '‡¶Æ‡¶æ‡¶¶‡¶æ‡¶∞‡¶ö‡ßã‡¶¶', '‡¶ï‡ßÅ‡¶§‡ßç‡¶§‡¶æ‡¶∞', '‡¶ú‡¶æ‡¶∞‡¶ú', '‡¶§‡ßã‡¶∞', '‡¶™‡ßã‡¶≤‡¶æ', '‡¶∂‡¶æ‡¶≤‡¶æ‡¶∞', '‡¶™‡ßã‡¶≤‡¶æ‡¶∞‡¶æ', '‡¶∂‡ßÅ‡¶Ø‡¶º‡ßã‡¶∞‡ßá‡¶∞', '‡¶Æ‡¶æ‡¶¶‡¶æ‡¶∞', '‡¶¨‡¶æ‡¶ö‡ßç‡¶ö‡¶æ‡¶∞‡¶æ'],\n",
        "            'Religious Hate': ['‡¶Æ‡ßÅ‡¶∏‡¶≤‡¶ø‡¶Æ', '‡¶π‡¶ø‡¶®‡ßç‡¶¶‡ßÅ', '‡¶á‡¶π‡ßÅ‡¶¶‡¶ø', '‡¶Æ‡ßÅ‡¶∏‡¶≤‡¶Æ‡¶æ‡¶®‡¶¶‡ßá‡¶∞', '‡¶ó‡¶ú‡¶¨', '‡¶ß‡¶∞‡ßç‡¶Æ', '‡¶π‡¶ø‡¶®‡ßç‡¶¶‡ßÅ‡¶¶‡ßá‡¶∞', '‡¶Æ‡ßÅ‡¶∏‡¶≤‡¶Æ‡¶æ‡¶®', '‡¶á‡¶∏‡¶≤‡¶æ‡¶Æ',\n",
        "                              '‡¶ï‡¶æ‡¶´‡ßá‡¶∞', '‡¶Æ‡¶∏‡¶ú‡¶ø‡¶¶', '‡¶ß‡¶∞‡ßç‡¶Æ‡ßÄ‡¶Ø‡¶º', '‡¶á‡¶π‡ßÅ‡¶¶‡¶ø‡¶∞‡¶æ', '‡¶Æ‡ßã‡¶≤‡ßç‡¶≤‡¶æ‡¶∞‡¶æ', '‡¶Ü‡¶≤‡ßç‡¶≤‡¶æ‡¶π‡¶∞', '‡¶π‡¶ø‡¶®‡ßç‡¶¶‡ßÅ‡¶∞', '‡¶Ü‡¶≤‡ßç‡¶≤‡¶æ‡¶π', '‡¶á‡¶π‡ßÅ‡¶¶‡ßÄ', '‡¶Æ‡ßÅ‡¶∏‡¶≤‡¶ø‡¶Æ‡¶∞‡¶æ', '‡¶ï‡¶æ‡¶´‡ßá‡¶∞‡¶¶‡ßá‡¶∞'],\n",
        "            'Political Hate': ['‡¶≠‡ßã‡¶ü', '‡¶¨‡¶ø‡¶è‡¶®‡¶™‡¶ø', '‡¶Ü‡¶ì‡¶Ø‡¶º‡¶æ‡¶Æ‡ßÄ', '‡¶≤‡ßÄ‡¶ó', '‡¶∏‡¶∞‡¶ï‡¶æ‡¶∞', '‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶®', '‡¶¨‡¶ø‡¶è‡¶®‡¶™‡¶ø‡¶∞', '‡¶≤‡ßÄ‡¶ó‡ßá‡¶∞', '‡¶π‡¶æ‡¶∏‡¶ø‡¶®‡¶æ',\n",
        "                              '‡¶∏‡¶∞‡¶ï‡¶æ‡¶∞‡ßá‡¶∞', '‡¶Ö‡¶¨‡ßà‡¶ß', '‡¶ú‡¶®‡¶ó‡¶£', '‡¶ï‡¶∞‡ßá', '‡¶™‡¶æ‡¶∞‡ßç‡¶ü‡¶ø', '‡¶¶‡¶≤', '‡¶ö‡ßã‡¶∞', '‡¶∞‡¶æ‡¶ú‡¶®‡ßÄ‡¶§‡¶ø'],\n",
        "            'Sexism': ['‡¶®‡¶æ‡¶∞‡ßÄ', '‡¶™‡¶∞‡¶ï‡¶ø‡¶Ø‡¶º‡¶æ', '‡¶Æ‡¶π‡¶ø‡¶≤‡¶æ‡¶¶‡ßá‡¶∞', '‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑‡¶¶‡ßá‡¶∞', '‡¶π‡¶ø‡¶ú‡¶∞‡¶æ', '‡¶Æ‡¶π‡¶ø‡¶≤‡¶æ', '‡¶Æ‡¶π‡¶ø‡¶≤‡¶æ‡¶ï‡ßá', '‡¶¨‡¶ø‡¶Ø‡¶º‡ßá‡¶∞',\n",
        "                      '‡¶Æ‡ßá‡¶Ø‡¶º‡ßá‡¶∞', '‡¶Æ‡ßá‡¶Ø‡¶º‡ßá', '‡¶®‡¶æ‡¶∞‡ßÄ‡¶∞‡¶æ', '‡¶Æ‡ßá‡¶Ø‡¶º‡ßá‡¶¶‡ßá‡¶∞', '‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑'],\n",
        "            'Abusive': ['‡¶¶‡¶æ‡¶≤‡¶æ‡¶≤', '‡¶ü‡¶ø‡¶≠‡¶ø', '‡¶´‡¶æ‡¶≤‡¶§‡ßÅ', '‡¶ö‡ßã‡¶∞', '‡¶ß‡¶®‡ßç‡¶Ø‡¶¨‡¶æ‡¶¶', '‡¶∏‡¶Æ‡¶Ø‡¶º', '‡¶Æ‡¶ø‡¶•‡ßç‡¶Ø‡¶æ', '‡¶™‡¶æ‡¶ó‡¶≤', '‡¶ú‡ßÅ‡¶§‡¶æ',\n",
        "                       '‡¶≤‡¶ú‡ßç‡¶ú‡¶æ', '‡¶®‡¶ø‡¶â‡¶ú', '‡¶Ü‡¶Æ‡¶ø‡¶®', '‡¶è‡¶¶‡ßá‡¶∞', '‡¶¶‡¶æ‡¶≤‡¶æ‡¶≤‡¶ø', '‡¶∏‡¶æ‡¶Ç‡¶¨‡¶æ‡¶¶‡¶ø‡¶ï', '‡¶è‡¶∞‡¶æ']\n",
        "        }\n",
        "\n",
        "        self.all_keywords = []\n",
        "        for words in self.label_keywords.values():\n",
        "            self.all_keywords.extend(words)\n",
        "        self.all_keywords = list(set(self.all_keywords))\n",
        "\n",
        "    def extract_features(self, texts):\n",
        "        features = []\n",
        "        for text in texts:\n",
        "            text_lower = str(text).lower()\n",
        "            feature_dict = {}\n",
        "\n",
        "            # Individual keyword counts\n",
        "            for keyword in self.all_keywords[:50]:  # Top 50 keywords\n",
        "                feature_dict[f'kw_{keyword}'] = text_lower.count(keyword)\n",
        "\n",
        "            # Label-specific aggregated features\n",
        "            for label, keywords in self.label_keywords.items():\n",
        "                total_count = sum(text_lower.count(kw.lower()) for kw in keywords)\n",
        "                feature_dict[f'label_{label}_total'] = total_count\n",
        "                feature_dict[f'label_{label}_ratio'] = total_count / len(text.split()) if len(text.split()) > 0 else 0\n",
        "\n",
        "            # Text statistics\n",
        "            feature_dict['text_length'] = len(text)\n",
        "            feature_dict['word_count'] = len(text.split())\n",
        "            feature_dict['avg_word_length'] = np.mean([len(word) for word in text.split()]) if text.split() else 0\n",
        "\n",
        "            features.append(feature_dict)\n",
        "\n",
        "        return pd.DataFrame(features).fillna(0)\n",
        "\n",
        "# Initialize keyword extractor\n",
        "keyword_extractor = KeywordFeatureExtractor()\n",
        "\n",
        "# Convert DataFrames to HuggingFace Datasets\n",
        "print(\"Converting to HuggingFace datasets...\")\n",
        "\n",
        "train_df_renamed = train_df.copy()\n",
        "val_df_renamed = val_df.copy()\n",
        "test_df_renamed = test_df.copy()\n",
        "\n",
        "# Remove 'label' column if it exists, then rename 'labels' -> 'label'\n",
        "for df in [train_df_renamed, val_df_renamed, test_df_renamed]:\n",
        "    if \"label\" in df.columns:\n",
        "        df.drop(columns=[\"label\"], inplace=True)\n",
        "    if \"labels\" in df.columns:\n",
        "        df.rename(columns={\"labels\": \"label\"}, inplace=True)\n",
        "\n",
        "\n",
        "# Add ID column if not present\n",
        "if 'id' not in train_df_renamed.columns:\n",
        "    train_df_renamed['id'] = range(len(train_df_renamed))\n",
        "if 'id' not in val_df_renamed.columns:\n",
        "    val_df_renamed['id'] = range(len(val_df_renamed))\n",
        "if 'id' not in test_df_renamed.columns:\n",
        "    test_df_renamed['id'] = range(len(test_df_renamed))\n",
        "\n",
        "# Convert to HuggingFace datasets\n",
        "train_dataset = Dataset.from_pandas(train_df_renamed)\n",
        "validation_dataset = Dataset.from_pandas(val_df_renamed)\n",
        "test_dataset = Dataset.from_pandas(test_df_renamed)\n",
        "\n",
        "# Create DatasetDict\n",
        "raw_datasets = DatasetDict({\n",
        "    \"train\": train_dataset,\n",
        "    \"validation\": validation_dataset,\n",
        "    \"test\": test_dataset\n",
        "})\n",
        "\n",
        "print(f\"Dataset sizes:\")\n",
        "print(f\"Train: {len(raw_datasets['train'])}\")\n",
        "print(f\"Validation: {len(raw_datasets['validation'])}\")\n",
        "print(f\"Test: {len(raw_datasets['test'])}\")\n",
        "\n",
        "for key in raw_datasets.keys():\n",
        "    logger.info(f\"loaded dataset for {key}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-69u1xeZzcGv"
      },
      "outputs": [],
      "source": [
        "# Labels\n",
        "label_list = raw_datasets[\"train\"].unique(\"label\")\n",
        "print(f\"Unique labels: {label_list}\")\n",
        "label_list.sort()  # sort the labels\n",
        "num_labels = len(label_list)\n",
        "print(f\"Number of labels: {num_labels}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1SWSOEd07pP"
      },
      "outputs": [],
      "source": [
        "config = AutoConfig.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_labels,\n",
        "    finetuning_task=None,\n",
        "    cache_dir=None,\n",
        "    revision=\"main\",\n",
        "    use_auth_token=None,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    cache_dir=None,\n",
        "    use_fast=True,\n",
        "    revision=\"main\",\n",
        "    use_auth_token=None,\n",
        ")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    from_tf=bool(\".ckpt\" in model_name),\n",
        "    config=config,\n",
        "    cache_dir=None,\n",
        "    revision=\"main\",\n",
        "    use_auth_token=None,\n",
        "    ignore_mismatched_sizes=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsnCMbhL0-dh"
      },
      "outputs": [],
      "source": [
        "non_label_column_names = [name for name in raw_datasets[\"train\"].column_names if name != \"label\"]\n",
        "print(f\"Non-label columns: {non_label_column_names}\")\n",
        "\n",
        "# Find the text column\n",
        "sentence1_key = 'text' if 'text' in non_label_column_names else 'sentence' if 'sentence' in non_label_column_names else non_label_column_names[1]\n",
        "print(f\"Using text column: {sentence1_key}\")\n",
        "\n",
        "# Padding strategy\n",
        "padding = \"max_length\"\n",
        "\n",
        "# Label mapping\n",
        "label_to_id = None\n",
        "if (model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id):\n",
        "    label_name_to_id = {k.lower(): v for k, v in model.config.label2id.items()}\n",
        "    if sorted(label_name_to_id.keys()) == sorted(label_list):\n",
        "        label_to_id = {i: int(label_name_to_id[label_list[i]]) for i in range(num_labels)}\n",
        "    else:\n",
        "        logger.warning(\n",
        "            \"Your model seems to have been trained with labels, but they don't match the dataset: \",\n",
        "            f\"model labels: {sorted(label_name_to_id.keys())}, dataset labels: {sorted(label_list)}.\"\n",
        "            \"\\nIgnoring the model labels as a result.\",)\n",
        "\n",
        "if label_to_id is not None:\n",
        "    model.config.label2id = label_to_id\n",
        "    model.config.id2label = {id: label for label, id in config.label2id.items()}\n",
        "\n",
        "# Set max sequence length\n",
        "max_seq_length = min(128, tokenizer.model_max_length)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # Tokenize the texts\n",
        "    args = (examples[sentence1_key],)\n",
        "    result = tokenizer(*args, padding=padding, max_length=max_seq_length, truncation=True)\n",
        "\n",
        "    # Map labels to IDs\n",
        "    if label_to_id is not None and \"label\" in examples:\n",
        "        result[\"label\"] = [(label_to_id[l] if l != -1 else -1) for l in examples[\"label\"]]\n",
        "    return result\n",
        "\n",
        "raw_datasets = raw_datasets.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    load_from_cache_file=True,\n",
        "    desc=\"Running tokenizer on dataset\",\n",
        ")\n",
        "\n",
        "print(\"Tokenization completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cccEZoCZ1Ee7"
      },
      "outputs": [],
      "source": [
        "if \"train\" not in raw_datasets:\n",
        "    raise ValueError(\"requires a train dataset\")\n",
        "train_dataset = raw_datasets[\"train\"]\n",
        "if max_train_samples is not None:\n",
        "    max_train_samples_n = min(len(train_dataset), max_train_samples)\n",
        "    train_dataset = train_dataset.select(range(max_train_samples_n))\n",
        "\n",
        "print(f\"Final train dataset: {len(train_dataset)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRzc6d3G1Jio"
      },
      "outputs": [],
      "source": [
        "if \"validation\" not in raw_datasets:\n",
        "    raise ValueError(\"requires a validation dataset\")\n",
        "eval_dataset = raw_datasets[\"validation\"]\n",
        "if max_eval_samples is not None:\n",
        "    max_eval_samples_n = min(len(eval_dataset), max_eval_samples)\n",
        "    eval_dataset = eval_dataset.select(range(max_eval_samples_n))\n",
        "\n",
        "print(f\"Final eval dataset: {len(eval_dataset)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YS-8bDNl1ME1"
      },
      "outputs": [],
      "source": [
        "if \"test\" not in raw_datasets:\n",
        "    raise ValueError(\"requires a test dataset\")\n",
        "predict_dataset = raw_datasets[\"test\"]\n",
        "if max_predict_samples is not None:\n",
        "    max_predict_samples_n = min(len(predict_dataset), max_predict_samples)\n",
        "    predict_dataset = predict_dataset.select(range(max_predict_samples_n))\n",
        "\n",
        "print(f\"Final predict dataset: {len(predict_dataset)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxHAUyEp1U1R"
      },
      "outputs": [],
      "source": [
        "for index in random.sample(range(len(train_dataset)), 3):\n",
        "    logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRSUw9kb1ZCg"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "metric_accuracy = evaluate.load(\"accuracy\")\n",
        "metric_f1 = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
        "    preds = np.argmax(preds, axis=1)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = (preds == p.label_ids).astype(np.float32).mean().item()\n",
        "\n",
        "    # Calculate micro F1 (official metric)\n",
        "    micro_f1 = metric_f1.compute(predictions=preds, references=p.label_ids, average='micro')['f1']\n",
        "\n",
        "    # Calculate macro F1 for additional insight\n",
        "    macro_f1 = metric_f1.compute(predictions=preds, references=p.label_ids, average='macro')['f1']\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"micro_f1\": micro_f1,\n",
        "        \"macro_f1\": macro_f1\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXjAWgh-1bJW"
      },
      "outputs": [],
      "source": [
        "data_collator = default_data_collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGbl81N01dx1"
      },
      "outputs": [],
      "source": [
        "# Remove ID columns for training\n",
        "train_dataset = train_dataset.remove_columns(\"id\")\n",
        "eval_dataset = eval_dataset.remove_columns(\"id\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    callbacks=[early_stopping],\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05edhuML1f1b"
      },
      "outputs": [],
      "source": [
        "print(\"Starting training...\")\n",
        "train_result = trainer.train()\n",
        "metrics = train_result.metrics\n",
        "max_train_samples = (\n",
        "    max_train_samples if max_train_samples is not None else len(train_dataset)\n",
        ")\n",
        "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
        "\n",
        "print(\"Training completed!\")\n",
        "print(f\"Training metrics: {metrics}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this after training completes\n",
        "print(\"Making final predictions...\")\n",
        "predictions = trainer.predict(predict_dataset, metric_key_prefix=\"predict\").predictions\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Get test IDs and save predictions\n",
        "test_ids = [test_df_renamed.iloc[i]['id'] for i in range(len(predict_dataset))]\n",
        "output_predict_file = os.path.join(training_args.output_dir, \"subtask_1A.tsv\")\n",
        "\n",
        "with open(output_predict_file, \"w\", encoding='utf-8') as writer:\n",
        "    writer.write(\"id\\tlabel\\tmodel\\n\")\n",
        "    for i, pred_idx in enumerate(predictions):\n",
        "        pred_label = id2l[pred_idx]\n",
        "        writer.write(f\"{test_ids[i]}\\t{pred_label}\\thatebert\\n\")\n",
        "\n",
        "# Create ZIP\n",
        "import zipfile\n",
        "zip_file_path = \"subtask_1A_banglabert.zip\"\n",
        "with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n",
        "    zipf.write(output_predict_file, \"subtask_1A.tsv\")\n",
        "\n",
        "print(f\"BanglaBert predictions saved: {output_predict_file}\")"
      ],
      "metadata": {
        "id": "TEr3WfDSc8we"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLC7L5PJ1jz-"
      },
      "outputs": [],
      "source": [
        "trainer.save_model()\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "trainer.save_state()\n",
        "\n",
        "print(\"Model saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0f-1EDjx1sYA"
      },
      "outputs": [],
      "source": [
        "logger.info(\"*** Evaluate ***\")\n",
        "\n",
        "metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
        "\n",
        "max_eval_samples = (\n",
        "    max_eval_samples if max_eval_samples is not None else len(eval_dataset)\n",
        ")\n",
        "metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
        "\n",
        "trainer.log_metrics(\"eval\", metrics)\n",
        "trainer.save_metrics(\"eval\", metrics)\n",
        "\n",
        "print(f\"Evaluation metrics: {metrics}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJhmqaOx2NnM"
      },
      "outputs": [],
      "source": [
        "# # Predictions\n",
        "# logger.info(\"*** Predict ***\")\n",
        "# try:\n",
        "#     # Get IDs before removing columns\n",
        "#     ids = predict_dataset['id']\n",
        "#     predict_dataset_clean = predict_dataset.remove_columns(\"id\")\n",
        "\n",
        "#     # Make predictions\n",
        "#     predictions = trainer.predict(predict_dataset_clean, metric_key_prefix=\"predict\").predictions\n",
        "#     predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "#     # Save predictions in required TSV format\n",
        "#     output_predict_file = os.path.join(training_args.output_dir, f\"subtask_1A.tsv\")\n",
        "\n",
        "#     if trainer.is_world_process_zero():\n",
        "#         with open(output_predict_file, \"w\", encoding='utf-8') as writer:\n",
        "#             logger.info(f\"***** Predict results *****\")\n",
        "#             writer.write(\"id\\tlabel\\tmodel\\n\")\n",
        "#             for index, pred_idx in enumerate(predictions):\n",
        "#                 pred_label = id2l[pred_idx]\n",
        "#                 writer.write(f\"{ids[index]}\\t{pred_label}\\t{model_name}\\n\")\n",
        "\n",
        "#         # Create submission ZIP\n",
        "#         zip_file_path = \"subtask_1A.zip\"\n",
        "#         with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n",
        "#             zipf.write(output_predict_file, \"subtask_1A.tsv\")\n",
        "#         print(f\"Submission file created: {zip_file_path}\")\n",
        "\n",
        "#         # Preview predictions\n",
        "#         print(f\"\\nFirst 10 predictions:\")\n",
        "#         print(\"ID | Predicted Label\")\n",
        "#         print(\"-\" * 40)\n",
        "#         for i in range(min(10, len(predictions))):\n",
        "#             pred_label = id2l[predictions[i]]\n",
        "#             print(f\"{ids[i]} | {pred_label}\")\n",
        "\n",
        "#         # ‚úÖ Calculate F1 scores if true labels are available\n",
        "#         from sklearn.metrics import f1_score\n",
        "\n",
        "#         try:\n",
        "#             true_labels = None\n",
        "\n",
        "#             # Check if predict_dataset has labels (before we removed columns)\n",
        "#             if 'label' in predict_dataset.column_names:\n",
        "#                 true_labels = predict_dataset['label']\n",
        "#                 print(f\"\\n‚úÖ Found true labels in predict_dataset\")\n",
        "#             # Check eval_dataset if available\n",
        "#             elif 'eval_dataset' in locals() and len(eval_dataset) == len(predictions):\n",
        "#                 true_labels = eval_dataset['label']\n",
        "#                 print(f\"\\n‚úÖ Found true labels in eval_dataset\")\n",
        "\n",
        "#             if true_labels is not None:\n",
        "#                 # Calculate F1 scores\n",
        "#                 micro_f1 = f1_score(true_labels, predictions, average='micro')\n",
        "#                 macro_f1 = f1_score(true_labels, predictions, average='macro')\n",
        "#                 weighted_f1 = f1_score(true_labels, predictions, average='weighted')\n",
        "\n",
        "#                 print(f\"\\n=== F1 SCORES ===\")\n",
        "#                 print(f\"üéØ Micro F1 (Official): {micro_f1:.4f}\")\n",
        "#                 print(f\"üìä Macro F1:            {macro_f1:.4f}\")\n",
        "#                 print(f\"‚öñÔ∏è  Weighted F1:         {weighted_f1:.4f}\")\n",
        "\n",
        "#                 # Per-class F1\n",
        "#                 per_class_f1 = f1_score(true_labels, predictions, average=None)\n",
        "#                 print(f\"\\nüìã Per-class F1:\")\n",
        "#                 for i, f1 in enumerate(per_class_f1):\n",
        "#                     print(f\"  {id2l[i]:12s}: {f1:.4f}\")\n",
        "\n",
        "#             else:\n",
        "#                 print(f\"\\n‚ö†Ô∏è  No true labels found - cannot calculate F1 scores\")\n",
        "\n",
        "#         except Exception as e:\n",
        "#             print(f\"\\n‚ùå Error calculating F1: {e}\")\n",
        "\n",
        "# except Exception as e:\n",
        "#     logger.error(f\"Error during prediction: {e}\")\n",
        "#     raise\n",
        "\n",
        "# # Download evaluation tools (do this once)\n",
        "# import os\n",
        "# if not os.path.exists(\"requirements.txt\"):\n",
        "#     !wget https://raw.githubusercontent.com/AridHasan/blp25_task1/main/requirements.txt -O requirements.txt\n",
        "#     !pip install -r requirements.txt\n",
        "\n",
        "# if not os.path.exists(\"scorer\"):\n",
        "#     os.makedirs(\"scorer\", exist_ok=True)\n",
        "#     !wget https://raw.githubusercontent.com/AridHasan/blp25_task1/main/scorer/task.py -O scorer/task.py\n",
        "\n",
        "# if not os.path.exists(\"format_checker\"):\n",
        "#     os.makedirs(\"format_checker\", exist_ok=True)\n",
        "#     !wget https://raw.githubusercontent.com/AridHasan/blp25_task1/main/format_checker/task.py -O format_checker/task.py\n",
        "\n",
        "# # Format verification\n",
        "# print(f\"\\n=== FORMAT VERIFICATION ===\")\n",
        "# print(f\"Output file: {output_predict_file}\")\n",
        "# print(\"Expected format: id\\\\tlabel\\\\tmodel\")\n",
        "# print(\"Running format checker...\")\n",
        "# !python format_checker/task.py -p {output_predict_file}\n",
        "\n",
        "# # Note: Replace 'path_to_gold_file.tsv' with actual gold standard file\n",
        "# print(\"\\nTo evaluate predictions against gold standard, run:\")\n",
        "# print(f\"python scorer/task.py -p {output_predict_file} -g path_to_gold_file.tsv\")\n",
        "# print(\"Official evaluation metric: micro-F1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91bORFeX2Rac"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import zipfile\n",
        "# import logging\n",
        "# import numpy as np\n",
        "\n",
        "# logger = logging.getLogger(__name__)\n",
        "# logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# output_dir = \"./xlm_roberta_recovery\"\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "# model_name = \"xlm-roberta\"\n",
        "# id2l = {0: 'None', 1: 'Abusive', 2: 'Sexism', 3: 'Religious Hate', 4: 'Political Hate', 5: 'Profane'}\n",
        "\n",
        "# # ======= PREDICTION =======\n",
        "# logger.info(\"*** Predict ***\")\n",
        "\n",
        "# ids = predict_dataset['id']\n",
        "# predict_dataset_clean = predict_dataset.remove_columns(\"id\")\n",
        "\n",
        "# predictions = trainer.predict(predict_dataset_clean, metric_key_prefix=\"predict\").predictions\n",
        "# predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "# # Save predictions TSV\n",
        "# output_predict_file = os.path.join(output_dir, \"subtask_1A.tsv\")\n",
        "# with open(output_predict_file, \"w\", encoding=\"utf-8\") as f:\n",
        "#     f.write(\"id\\tlabel\\tmodel\\n\")\n",
        "#     for idx, pred_idx in enumerate(predictions):\n",
        "#         pred_label = id2l[pred_idx]\n",
        "#         f.write(f\"{ids[idx]}\\t{pred_label}\\t{model_name}\\n\")\n",
        "\n",
        "# logger.info(f\"Predictions saved: {output_predict_file}\")\n",
        "\n",
        "# # ======= ZIP PREDICTIONS =======\n",
        "# zip_file_path = os.path.join(output_dir, \"subtask_1A.zip\")\n",
        "# with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n",
        "#     zipf.write(output_predict_file, \"subtask_1A.tsv\")\n",
        "\n",
        "# logger.info(f\"Submission file created and zipped: {zip_file_path}\")\n",
        "\n",
        "# # ======= PREVIEW =======\n",
        "# print(\"\\nFirst 10 predictions:\")\n",
        "# import pandas as pd\n",
        "# print(pd.read_csv(output_predict_file, sep=\"\\t\").head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Saj4iI-q2VHm"
      },
      "outputs": [],
      "source": [
        "# # ‚úÖ Show only ID + predicted label for preview\n",
        "# print(f\"\\nFirst 10 predictions (Required Format):\")\n",
        "# print(\"Format: ID | Predicted Label\")\n",
        "# print(\"-\" * 40)\n",
        "# for i in range(min(10, len(predictions))):\n",
        "#     pred_label = id2l[predictions[i]]\n",
        "#     print(f\"{ids[i]} | {pred_label}\")\n",
        "\n",
        "\n",
        "# print(f\"\\n=== FORMAT VERIFICATION ===\")\n",
        "# print(f\"Output file: {output_predict_file}\")\n",
        "# print(\"Expected format: id\\\\tlabel\\\\tmodel\")\n",
        "# print(\"To verify format compliance, run:\")\n",
        "# print(f\"python format_checker/task.py -p {output_predict_file}\")\n",
        "# print(\"\\nTo evaluate predictions, run:\")\n",
        "# print(f\"python scorer/task.py --gold-file-path=<gold_file> --pred-file-path={output_predict_file}\")\n",
        "# print(f\"Official evaluation metric: micro-F1\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFaIPjjuc9OR"
      },
      "outputs": [],
      "source": [
        "# !python format_checker/task.py -p /content/xlm_roberta_recovery/subtask_1A.tsv\n",
        "# !python scorer/task.py -p /content/xlm_roberta_recovery/subtask_1A.tsv -g /content/dev.tsv\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOy-IJm2Fwxd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}